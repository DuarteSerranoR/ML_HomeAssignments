{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, max_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, normalize, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "\n",
    "import pickle\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def printRegStatistics(truth, preds):\n",
    "    print(\"The RVE is: \", explained_variance_score(truth, preds))\n",
    "    print(\"The mse is: \", mean_squared_error(truth, preds, squared=False))\n",
    "    corr, pval = pearsonr(truth, preds)\n",
    "    print(\"The Correlation Score is is: %6.4f (p-value=%e)\\n\"%(corr,pval))\n",
    "    print(\"The Maximum Error is is: \", max_error(truth, preds))\n",
    "    print(\"The Mean Absolute Error is: \", mean_absolute_error(truth, preds))\n",
    "    \n",
    "def one_hot_encode_categorical_features(all_features: pd.DataFrame) -> pd.DataFrame:\n",
    "    is_categorical = lambda f: all_features[f].apply(lambda x: not isinstance(x, float) or x.is_integer()).all()\n",
    "    categorical_cols = [f for f in all_features if is_categorical(f)]\n",
    "    other_cols = [f for f in all_features if not is_categorical(f)]\n",
    "    \n",
    "    encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    categorical_matrix = all_features[categorical_cols].to_numpy().astype(int).astype(str)\n",
    "    categorical_encoded_matrix = encoder.fit_transform(categorical_matrix).toarray()\n",
    "    \n",
    "    headers = []\n",
    "    for base_name, categories in zip(categorical_cols, encoder.categories_):\n",
    "        for c in categories:\n",
    "            headers.append(base_name + \"_\" + c)\n",
    "        # print(f\"{base_name} has {len(categories)} categories\")\n",
    "    \n",
    "    return pd.concat([all_features[other_cols], pd.DataFrame(data=categorical_encoded_matrix, columns=headers)], axis=1)\n",
    "\n",
    "def is_float(element: any) -> bool:\n",
    "    #If you expect None to be passed:\n",
    "    if element is None: \n",
    "        return False\n",
    "    try:\n",
    "        float(element)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "from threading import Lock, Thread\n",
    "\n",
    "class SupportedModelType(Enum):\n",
    "\n",
    "    LINEAR_REGRESSION=0\n",
    "    SVR=1\n",
    "    RANDOM_FOREST_REGRESSOR=2\n",
    "    EXTRA_TREES_REGRESSOR=3\n",
    "\n",
    "\n",
    "class CustomSVR():\n",
    "\n",
    "    __predictive_function_lock__: Lock = Lock()\n",
    "    __prediction_array_lock__: Lock = Lock()\n",
    "    __prediction_array__: np.array = np.array([])\n",
    "\n",
    "    __country_population_model__: SVR\n",
    "    __fertility_rate_model__: SVR\n",
    "    __life_expectancy_model__: SVR\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                    kernel: str = \"rbf\",\n",
    "                    degree: int = 3,\n",
    "                    gamma: str = \"scale\",\n",
    "                    coef0: float = 0,\n",
    "                    tol: float = 0.001,\n",
    "                    C: float = 1,\n",
    "                    epsilon: float = 0.1,\n",
    "                    shrinking: bool = True,\n",
    "                    cache_size: int = 200,\n",
    "                    verbose: bool = False,\n",
    "                    max_iter: int = -1\n",
    "                ):\n",
    "\n",
    "        self.__country_population_model__ = SVR(\n",
    "            kernel=kernel,\n",
    "            degree=degree,\n",
    "            gamma=gamma,\n",
    "            coef0=coef0,\n",
    "            tol=tol,\n",
    "            C=C,\n",
    "            epsilon=epsilon,\n",
    "            shrinking=shrinking,\n",
    "            cache_size=cache_size,\n",
    "            verbose=verbose,\n",
    "            max_iter=max_iter\n",
    "        )\n",
    "\n",
    "        self.__fertility_rate_model__ = SVR(\n",
    "            kernel=kernel,\n",
    "            degree=degree,\n",
    "            gamma=gamma,\n",
    "            coef0=coef0,\n",
    "            tol=tol,\n",
    "            C=C,\n",
    "            epsilon=epsilon,\n",
    "            shrinking=shrinking,\n",
    "            cache_size=cache_size,\n",
    "            verbose=verbose,\n",
    "            max_iter=max_iter\n",
    "        )\n",
    "\n",
    "        self.__life_expectancy_model__ = SVR(\n",
    "            kernel=kernel,\n",
    "            degree=degree,\n",
    "            gamma=gamma,\n",
    "            coef0=coef0,\n",
    "            tol=tol,\n",
    "            C=C,\n",
    "            epsilon=epsilon,\n",
    "            shrinking=shrinking,\n",
    "            cache_size=cache_size,\n",
    "            verbose=verbose,\n",
    "            max_iter=max_iter\n",
    "        )\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        self.__country_population_model__.fit(x,y[:,0])\n",
    "        self.__fertility_rate_model__.fit(x,y[:,1])\n",
    "        self.__life_expectancy_model__.fit(x,y[:,2])\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        self.__predictive_function_lock__.acquire()\n",
    "        \n",
    "        t_cp = Thread(target=self.__country_population_predict__, args=(x))\n",
    "        t_cp.start()\n",
    "        t_fr = Thread(target=self.__fertility_rate_predict__, args=(x))\n",
    "        t_fr.start()\n",
    "        t_le = Thread(target=self.__life_expectancy_predict__, args=(x))\n",
    "        t_le.start()\n",
    "\n",
    "        t_cp.join()\n",
    "        t_fr.join()\n",
    "        t_le.join()\n",
    "\n",
    "        self.__predictive_function_lock__.release()\n",
    "        return np.array(self.__prediction_array__)\n",
    "\n",
    "\n",
    "\n",
    "    def __country_population_predict__(self,x):\n",
    "\n",
    "        pred_cp = self.__country_population_model__.predict(x)\n",
    "\n",
    "        self.__prediction_array_lock__.acquire()\n",
    "        self.__prediction_array__[0] = pred_cp[0]\n",
    "        self.__prediction_array_lock__.release()\n",
    "\n",
    "    def __fertility_rate_predict__(self,x):\n",
    "\n",
    "        pred_fr = self.__fertility_rate_model__.predict(x)\n",
    "\n",
    "        self.__prediction_array_lock__.acquire()\n",
    "        self.__prediction_array__[1] = pred_fr[0]\n",
    "        self.__prediction_array_lock__.release()\n",
    "\n",
    "    def __life_expectancy_predict__(self,x):\n",
    "\n",
    "        pred_le = self.__life_expectancy_model__.predict(x)\n",
    "\n",
    "        self.__prediction_array_lock__.acquire()\n",
    "        self.__prediction_array__[2] = pred_le[0]\n",
    "        self.__prediction_array_lock__.release()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def kfold_val(model_type: SupportedModelType, target_metrics: list, X_raw_train_df: pd.DataFrame, X_raw_test_df: pd.DataFrame, Y_delta_df: pd.DataFrame, Y_raw_df: pd.DataFrame, hyper_parameters):\n",
    "\n",
    "    assert \"RVE\" in target_metrics or \"mse\" in target_metrics or \"CorrelationScore\" in target_metrics or \"MaximumError\" in target_metrics or \"MeanAbsoluteError\" in target_metrics\n",
    "\n",
    "    assert len(target_metrics) == 1 and target_metrics[0] == \"mse\" # Not implemented fitness evaluation\n",
    "\n",
    "    train_len = len(X_raw_train_df)\n",
    "    test_len = len(X_raw_test_df)\n",
    "\n",
    "    total_len = train_len + test_len\n",
    "\n",
    "    X_raw_dataset = pd.concat([X_raw_train_df,X_raw_test_df], axis=0)\n",
    "\n",
    "    assert len(X_raw_dataset) == len(Y_delta_df) == len(Y_raw_df)\n",
    "\n",
    "    best_model = None\n",
    "    best_hyperparams = None\n",
    "    best_hyperparams_possib = []\n",
    "    best_score = 99999999999999999999999.0\n",
    "\n",
    "    for p in hyper_parameters:\n",
    "\n",
    "        if model_type == SupportedModelType.LINEAR_REGRESSION:\n",
    "            fit_intercept,\\\n",
    "                normalize,\\\n",
    "                copy_X,\\\n",
    "                n_jobs,\\\n",
    "                positive = p\n",
    "\n",
    "        elif model_type == SupportedModelType.SVR:\n",
    "            kernel,\\\n",
    "                degree,\\\n",
    "                gamma,\\\n",
    "                coef0,\\\n",
    "                tol,\\\n",
    "                C,\\\n",
    "                epsilon,\\\n",
    "                shrinking,\\\n",
    "                cache_size,\\\n",
    "                verbose,\\\n",
    "                max_iter = p\n",
    "\n",
    "        elif model_type == SupportedModelType.RANDOM_FOREST_REGRESSOR:\n",
    "            n_estimators,\\\n",
    "                criterion,\\\n",
    "                max_depth,\\\n",
    "                min_samples_split,\\\n",
    "                min_samples_leaf,\\\n",
    "                min_weight_fraction_leaf,\\\n",
    "                max_features,\\\n",
    "                max_leaf_nodes,\\\n",
    "                min_impurity_decrease,\\\n",
    "                bootstrap,\\\n",
    "                oob_score,\\\n",
    "                n_jobs,\\\n",
    "                random_state,\\\n",
    "                verbose,\\\n",
    "                warm_start,\\\n",
    "                ccp_alpha,\\\n",
    "                max_samples = p\n",
    "        \n",
    "        elif model_type == SupportedModelType.EXTRA_TREES_REGRESSOR:\n",
    "            n_estimators,\\\n",
    "                criterion,\\\n",
    "                max_depth,\\\n",
    "                min_samples_split,\\\n",
    "                min_samples_leaf,\\\n",
    "                min_weight_fraction_leaf,\\\n",
    "                max_features,\\\n",
    "                max_leaf_nodes,\\\n",
    "                min_impurity_decrease,\\\n",
    "                bootstrap,\\\n",
    "                oob_score,\\\n",
    "                n_jobs,\\\n",
    "                random_state,\\\n",
    "                verbose,\\\n",
    "                warm_start,\\\n",
    "                ccp_alpha,\\\n",
    "                max_samples = p\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        ix_test_start = 0\n",
    "        score = 0\n",
    "        n_iter = 0\n",
    "        for ix_test_end in range(test_len,total_len,test_len):\n",
    "\n",
    "            n_iter+=1\n",
    "\n",
    "            X_train_df = pd.concat([X_raw_dataset[0:ix_test_start],X_raw_dataset[ix_test_end:]], axis=0)\n",
    "            X_test_df = X_raw_dataset[ix_test_start:ix_test_end]\n",
    "            X_test_np = X_test_df.to_numpy()\n",
    "\n",
    "            Y_delta_train_df = pd.concat([Y_delta_df[0:ix_test_start],Y_delta_df[ix_test_end:]], axis=0)\n",
    "            Y_raw_test_df = Y_raw_df[ix_test_start:ix_test_end]\n",
    "\n",
    "\n",
    "            if model_type == SupportedModelType.LINEAR_REGRESSION:\n",
    "                \n",
    "                model = LinearRegression(\n",
    "                        fit_intercept=fit_intercept,\n",
    "                        normalize=normalize,\n",
    "                        copy_X=copy_X,\n",
    "                        n_jobs=n_jobs,\n",
    "                        positive=positive\n",
    "                    )\n",
    "                \n",
    "                model.fit(X_train_df.to_numpy(), Y_delta_train_df.to_numpy())\n",
    "\n",
    "                truth = Y_raw_test_df.to_numpy()\n",
    "\n",
    "                predicted_total=[]\n",
    "                preds=[]\n",
    "                curr_cp_ix = X_test_df.columns.to_list().index(\"country_population_curr\")\n",
    "                curr_fr_ix = X_test_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "                curr_le_ix = X_test_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "                for x in X_test_np:\n",
    "                    y = model.predict(x.reshape(1, -1))[0]\n",
    "                    y += np.array([x[curr_cp_ix], x[curr_fr_ix], x[curr_le_ix]])\n",
    "                    preds.append(y)\n",
    "                    predicted_total.append(y[0] * y[1] * y[2])\n",
    "                predicted=np.array(preds)\n",
    "                predicted_total=np.array(predicted_total)\n",
    "\n",
    "                truth_total = [ t[0] * t[1] * t[2] for t in truth ]\n",
    "                truth_total = np.array(truth_total)\n",
    "\n",
    "                mse = mean_squared_error(truth_total, predicted_total)\n",
    "                score += mse\n",
    "\n",
    "            elif model_type == SupportedModelType.SVR:\n",
    "                \n",
    "                model = CustomSVR(\n",
    "                        kernel=kernel,\n",
    "                        degree=degree,\n",
    "                        gamma=gamma,\n",
    "                        coef0=coef0,\n",
    "                        tol=tol,\n",
    "                        C=C,\n",
    "                        epsilon=epsilon,\n",
    "                        shrinking=shrinking,\n",
    "                        cache_size=cache_size,\n",
    "                        verbose=verbose,\n",
    "                        max_iter=max_iter\n",
    "                    )\n",
    "                \n",
    "                model.fit(X_train_df.to_numpy(), Y_delta_train_df.to_numpy())\n",
    "\n",
    "                truth = Y_raw_test_df.to_numpy()\n",
    "\n",
    "                predicted_total=[]\n",
    "                preds=[]\n",
    "                curr_cp_ix = X_test_df.columns.to_list().index(\"country_population_curr\")\n",
    "                curr_fr_ix = X_test_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "                curr_le_ix = X_test_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "                for x in X_test_np:\n",
    "                    y = model.predict(x.reshape(1, -1))\n",
    "                    y += np.array([x[curr_cp_ix], x[curr_fr_ix], x[curr_le_ix]])\n",
    "                    preds.append(y)\n",
    "                    predicted_total.append(y[0] * y[1] * y[2])\n",
    "                predicted=np.array(preds)\n",
    "                predicted_total=np.array(predicted_total)\n",
    "\n",
    "                truth_total = [ t[0] * t[1] * t[2] for t in truth ]\n",
    "                truth_total = np.array(truth_total)\n",
    "\n",
    "                mse = mean_squared_error(truth_total, predicted_total)\n",
    "                score += mse\n",
    "\n",
    "            elif model_type == SupportedModelType.RANDOM_FOREST_REGRESSOR:\n",
    "                \n",
    "                model = RandomForestRegressor(\n",
    "                        n_estimators=n_estimators,\n",
    "                        criterion=criterion,\n",
    "                        max_depth=max_depth,\n",
    "                        min_samples_split=min_samples_split,\n",
    "                        min_samples_leaf=min_samples_leaf,\n",
    "                        min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                        max_features=max_features,\n",
    "                        max_leaf_nodes=max_leaf_nodes,\n",
    "                        min_impurity_decrease=min_impurity_decrease,\n",
    "                        bootstrap=bootstrap,\n",
    "                        oob_score=oob_score,\n",
    "                        n_jobs=n_jobs,\n",
    "                        random_state=random_state,\n",
    "                        verbose=verbose,\n",
    "                        warm_start=warm_start,\n",
    "                        ccp_alpha=ccp_alpha,\n",
    "                        max_samples=max_samples\n",
    "                    )\n",
    "                \n",
    "                model.fit(X_train_df.to_numpy(), Y_delta_train_df.to_numpy())\n",
    "\n",
    "                truth = Y_raw_test_df.to_numpy()\n",
    "\n",
    "                preds=[]\n",
    "                predicted_total=[]\n",
    "                curr_cp_ix = X_test_df.columns.to_list().index(\"country_population_curr\")\n",
    "                curr_fr_ix = X_test_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "                curr_le_ix = X_test_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "                for x in X_test_np:\n",
    "                    y = model.predict(x.reshape(1, -1))[0]\n",
    "                    y += np.array([x[curr_cp_ix], x[curr_fr_ix], x[curr_le_ix]])\n",
    "                    preds.append(y)\n",
    "                    predicted_total.append(y[0] * y[1] * y[2])\n",
    "                predicted=np.array(preds)\n",
    "                predicted_total=np.array(predicted_total)\n",
    "\n",
    "                truth_total = [ t[0] * t[1] * t[2] for t in truth ]\n",
    "                truth_total = np.array(truth_total)\n",
    "\n",
    "                mse = mean_squared_error(truth_total, predicted_total)\n",
    "                score += mse\n",
    "\n",
    "            elif model_type == SupportedModelType.EXTRA_TREES_REGRESSOR:\n",
    "                \n",
    "                model = ExtraTreesRegressor(\n",
    "                        n_estimators=n_estimators,\n",
    "                        criterion=criterion,\n",
    "                        max_depth=max_depth,\n",
    "                        min_samples_split=min_samples_split,\n",
    "                        min_samples_leaf=min_samples_leaf,\n",
    "                        min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                        max_features=max_features,\n",
    "                        max_leaf_nodes=max_leaf_nodes,\n",
    "                        min_impurity_decrease=min_impurity_decrease,\n",
    "                        bootstrap=bootstrap,\n",
    "                        oob_score=oob_score,\n",
    "                        n_jobs=n_jobs,\n",
    "                        random_state=random_state,\n",
    "                        verbose=verbose,\n",
    "                        warm_start=warm_start,\n",
    "                        ccp_alpha=ccp_alpha,\n",
    "                        max_samples=max_samples\n",
    "                    )\n",
    "                \n",
    "                model.fit(X_train_df.to_numpy(), Y_delta_train_df.to_numpy())\n",
    "\n",
    "                truth = Y_raw_test_df.to_numpy()\n",
    "\n",
    "                preds=[]\n",
    "                predicted_total=[]\n",
    "                curr_cp_ix = X_test_df.columns.to_list().index(\"country_population_curr\")\n",
    "                curr_fr_ix = X_test_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "                curr_le_ix = X_test_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "                for x in X_test_np:\n",
    "                    y = model.predict(x.reshape(1, -1))[0]\n",
    "                    y += np.array([x[curr_cp_ix], x[curr_fr_ix], x[curr_le_ix]])\n",
    "                    preds.append(y)\n",
    "                    predicted_total.append(y[0] * y[1] * y[2])\n",
    "                predicted=np.array(preds)\n",
    "                predicted_total=np.array(predicted_total)\n",
    "\n",
    "                truth_total = [ t[0] * t[1] * t[2] for t in truth ]\n",
    "                truth_total = np.array(truth_total)\n",
    "\n",
    "                mse = mean_squared_error(truth_total, predicted_total)\n",
    "                score += mse\n",
    "                \n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "\n",
    "            ix_test_start = ix_test_end\n",
    "        \n",
    "\n",
    "        score = score / n_iter\n",
    "\n",
    "        if score < best_score:\n",
    "            best_score = mse\n",
    "            best_hyperparams = p\n",
    "            best_hyperparams_possib = [best_hyperparams]\n",
    "            print(f\"new best score: {str(score)}\")\n",
    "        elif score == best_score:\n",
    "            best_hyperparams_possib.append(p)\n",
    "            print(f\"equally good score: {str(score)}\")\n",
    "        else:\n",
    "            print(f\"score: {str(score)}\")\n",
    "\n",
    "\n",
    "\n",
    "    if model_type == SupportedModelType.LINEAR_REGRESSION:\n",
    "        fit_intercept,\\\n",
    "            normalize,\\\n",
    "            copy_X,\\\n",
    "            n_jobs,\\\n",
    "            positive = best_hyperparams\n",
    "\n",
    "        best_model = LinearRegression(\n",
    "                fit_intercept=fit_intercept,\n",
    "                normalize=normalize,\n",
    "                copy_X=copy_X,\n",
    "                n_jobs=n_jobs,\n",
    "                positive=positive\n",
    "            )\n",
    "\n",
    "        best_model.fit(X_raw_dataset, Y_delta_df)\n",
    "\n",
    "    elif model_type == SupportedModelType.SVR:\n",
    "        kernel,\\\n",
    "            degree,\\\n",
    "            gamma,\\\n",
    "            coef0,\\\n",
    "            tol,\\\n",
    "            C,\\\n",
    "            epsilon,\\\n",
    "            shrinking,\\\n",
    "            cache_size,\\\n",
    "            verbose,\\\n",
    "            max_iter = best_hyperparams\n",
    "            \n",
    "        best_model = CustomSVR(\n",
    "            kernel=kernel,\n",
    "            degree=degree,\n",
    "            gamma=gamma,\n",
    "            coef0=coef0,\n",
    "            tol=tol,\n",
    "            C=C,\n",
    "            epsilon=epsilon,\n",
    "            shrinking=shrinking,\n",
    "            cache_size=cache_size,\n",
    "            verbose=verbose,\n",
    "            max_iter=max_iter\n",
    "        )\n",
    "\n",
    "        best_model.fit(X_raw_dataset, Y_delta_df)\n",
    "    \n",
    "    elif model_type == SupportedModelType.RANDOM_FOREST_REGRESSOR:\n",
    "        n_estimators,\\\n",
    "            criterion,\\\n",
    "            max_depth,\\\n",
    "            min_samples_split,\\\n",
    "            min_samples_leaf,\\\n",
    "            min_weight_fraction_leaf,\\\n",
    "            max_features,\\\n",
    "            max_leaf_nodes,\\\n",
    "            min_impurity_decrease,\\\n",
    "            bootstrap,\\\n",
    "            oob_score,\\\n",
    "            n_jobs,\\\n",
    "            random_state,\\\n",
    "            verbose,\\\n",
    "            warm_start,\\\n",
    "            ccp_alpha,\\\n",
    "            max_samples = best_hyperparams\n",
    "        \n",
    "        best_model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                criterion=criterion,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                max_features=max_features,\n",
    "                max_leaf_nodes=max_leaf_nodes,\n",
    "                min_impurity_decrease=min_impurity_decrease,\n",
    "                bootstrap=bootstrap,\n",
    "                oob_score=oob_score,\n",
    "                n_jobs=n_jobs,\n",
    "                random_state=random_state,\n",
    "                verbose=verbose,\n",
    "                warm_start=warm_start,\n",
    "                ccp_alpha=ccp_alpha,\n",
    "                max_samples=max_samples\n",
    "            )\n",
    "\n",
    "    elif model_type == SupportedModelType.EXTRA_TREES_REGRESSOR:\n",
    "        n_estimators,\\\n",
    "            criterion,\\\n",
    "            max_depth,\\\n",
    "            min_samples_split,\\\n",
    "            min_samples_leaf,\\\n",
    "            min_weight_fraction_leaf,\\\n",
    "            max_features,\\\n",
    "            max_leaf_nodes,\\\n",
    "            min_impurity_decrease,\\\n",
    "            bootstrap,\\\n",
    "            oob_score,\\\n",
    "            n_jobs,\\\n",
    "            random_state,\\\n",
    "            verbose,\\\n",
    "            warm_start,\\\n",
    "            ccp_alpha,\\\n",
    "            max_samples = best_hyperparams\n",
    "        \n",
    "        best_model = ExtraTreesRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                criterion=criterion,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                max_features=max_features,\n",
    "                max_leaf_nodes=max_leaf_nodes,\n",
    "                min_impurity_decrease=min_impurity_decrease,\n",
    "                bootstrap=bootstrap,\n",
    "                oob_score=oob_score,\n",
    "                n_jobs=n_jobs,\n",
    "                random_state=random_state,\n",
    "                verbose=verbose,\n",
    "                warm_start=warm_start,\n",
    "                ccp_alpha=ccp_alpha,\n",
    "                max_samples=max_samples\n",
    "            )\n",
    "        \n",
    "        best_model.fit(X_raw_dataset.to_numpy(), Y_delta_df.to_numpy())\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return best_model, best_hyperparams_possib, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_df = pd.read_csv(\"./country_population.csv\").sort_values(by=\"Country Name\",ascending=True).reset_index(drop=True)\n",
    "fr_df = pd.read_csv(\"./fertility_rate.csv\").sort_values(by=\"Country Name\",ascending=True).reset_index(drop=True)\n",
    "le_df = pd.read_csv(\"./life_expectancy.csv\").sort_values(by=\"Country Name\",ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Indicator Name</th>\n",
       "      <th>Indicator Code</th>\n",
       "      <th>1960</th>\n",
       "      <th>1961</th>\n",
       "      <th>1962</th>\n",
       "      <th>1963</th>\n",
       "      <th>1964</th>\n",
       "      <th>1965</th>\n",
       "      <th>...</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>Population, total</td>\n",
       "      <td>SP.POP.TOTL</td>\n",
       "      <td>8996351.00</td>\n",
       "      <td>9166764.00</td>\n",
       "      <td>9345868.00</td>\n",
       "      <td>9533954.00</td>\n",
       "      <td>9731361.00</td>\n",
       "      <td>9938414.00</td>\n",
       "      <td>...</td>\n",
       "      <td>26616792.00</td>\n",
       "      <td>27294031.00</td>\n",
       "      <td>28004331.00</td>\n",
       "      <td>28803167.00</td>\n",
       "      <td>29708599.00</td>\n",
       "      <td>30696958.00</td>\n",
       "      <td>31731688.00</td>\n",
       "      <td>32758020.00</td>\n",
       "      <td>33736494.00</td>\n",
       "      <td>34656032.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country Name Country Code     Indicator Name Indicator Code       1960  \\\n",
       "0  Afghanistan          AFG  Population, total    SP.POP.TOTL 8996351.00   \n",
       "\n",
       "        1961       1962       1963       1964       1965  ...        2007  \\\n",
       "0 9166764.00 9345868.00 9533954.00 9731361.00 9938414.00  ... 26616792.00   \n",
       "\n",
       "         2008        2009        2010        2011        2012        2013  \\\n",
       "0 27294031.00 28004331.00 28803167.00 29708599.00 30696958.00 31731688.00   \n",
       "\n",
       "         2014        2015        2016  \n",
       "0 32758020.00 33736494.00 34656032.00  \n",
       "\n",
       "[1 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cp_df[\"Country Name\"].to_list() == fr_df[\"Country Name\"].to_list() == le_df[\"Country Name\"].to_list()\n",
    "assert cp_df[\"Country Code\"].to_list() == fr_df[\"Country Code\"].to_list() == le_df[\"Country Code\"].to_list()\n",
    "countries_df = cp_df[\"Country Code\"].to_list()\n",
    "#display(countries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs and data theme, we can assume that these 3 features we want to predict have some correlation between each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used GDP dataset -> https://data.worldbank.org/indicator/NY.GDP.MKTP.CD -> the csv is corrupted when downloaded from the website, need manual changes (delete non-corresponding lines at the beggining)\n",
    "# also available but without all countries -> https://www.kaggle.com/datasets/rinichristy/countries-gdp-19602020\n",
    "#  and by continents -> https://www.kaggle.com/datasets/holoong9291/gdp-of-all-countries19602020\n",
    "\n",
    "gdp_df = pd.read_csv(\"./imported_datasets/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_4770391.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_list = []\n",
    "Y_delta_list = []\n",
    "Y_roc_list = [] # Rate of Change\n",
    "\n",
    "X_list = []\n",
    "\n",
    "n_prev_inputs = 2\n",
    "assert n_prev_inputs >= 1\n",
    "\n",
    "Y_labels = [\"country_population_pred\", \"fertility_rate_pred\", \"life_expectancy_pred\"]\n",
    "Y_delta_labels = [\"country_population_delta\", \"fertility_rate_delta\", \"life_expectancy_delta\"]\n",
    "Y_roc_labels = [\"country_population_roc\", \"fertility_rate_roc\", \"life_expectancy_roc\"]\n",
    "\n",
    "prev_countrypop_tags = []\n",
    "prev_fertrate_tags = []\n",
    "prev_lifeexp_tags = []\n",
    "for ix in range(n_prev_inputs-1,-1,-1):\n",
    "    prev_countrypop_tags.append(\"country_population_prev_\"+str(ix))\n",
    "    prev_fertrate_tags.append(\"fertility_rate_prev_\"+str(ix))\n",
    "    prev_lifeexp_tags.append(\"life_expectancy_prev_\"+str(ix))\n",
    "\n",
    "X_labels = []\n",
    "X_labels.extend(prev_countrypop_tags)\n",
    "X_labels.append(\"country_population_curr\")\n",
    "X_labels.extend(prev_fertrate_tags)\n",
    "X_labels.append(\"fertility_rate_curr\")\n",
    "X_labels.extend(prev_lifeexp_tags)\n",
    "X_labels.append(\"life_expectancy_curr\")\n",
    "\n",
    "X_labels.append(\"country_code\")\n",
    "X_labels.append(\"gdp_curr\")\n",
    "\n",
    "\n",
    "\n",
    "for curr_row_ix in range(0, len(cp_df)):\n",
    "\n",
    "    gdp_country_series = gdp_df.loc[gdp_df[\"Country Code\"] == cp_df.iloc[curr_row_ix][\"Country Code\"]]\n",
    "\n",
    "    country_code = gdp_df.iloc[curr_row_ix][\"Country Code\"]\n",
    "\n",
    "    cp_row_years = cp_df.drop([\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\"], axis=1).iloc[curr_row_ix]\n",
    "    fr_row_years = fr_df.drop([\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\"], axis=1).iloc[curr_row_ix]\n",
    "    le_row_years = le_df.drop([\"Country Name\",\"Country Code\",\"Indicator Name\",\"Indicator Code\"], axis=1).iloc[curr_row_ix]\n",
    "\n",
    "    for curr_column_ix in range(n_prev_inputs+1,len(cp_row_years.to_numpy())):\n",
    "\n",
    "        assert cp_row_years.axes[0][curr_column_ix-1] == fr_row_years.axes[0][curr_column_ix-1] == le_row_years.axes[0][curr_column_ix-1]\n",
    "        year = cp_row_years.axes[0][curr_column_ix-1]\n",
    "\n",
    "        gdp_vals = gdp_country_series[year].values\n",
    "        assert len(gdp_vals) == 1\n",
    "\n",
    "        gdp = gdp_vals[0]\n",
    "\n",
    "        x = []\n",
    "\n",
    "        for ix in range(n_prev_inputs-1,-1,-1):\n",
    "            var = cp_row_years.iloc[curr_column_ix-ix-2]\n",
    "            x.append(var)\n",
    "\n",
    "        cp_curr = cp_row_years.iloc[curr_column_ix-1]\n",
    "        cp_pred = cp_row_years.iloc[curr_column_ix]\n",
    "\n",
    "        x.append(cp_curr)\n",
    "\n",
    "\n",
    "        for ix in range(n_prev_inputs-1,-1,-1):\n",
    "            var = fr_row_years.iloc[curr_column_ix-ix-2]\n",
    "            x.append(var)\n",
    "        \n",
    "        fr_curr = fr_row_years.iloc[curr_column_ix-1]\n",
    "        fr_pred = fr_row_years.iloc[curr_column_ix]\n",
    "\n",
    "        x.append(fr_curr)\n",
    "\n",
    "\n",
    "        for ix in range(n_prev_inputs-1,-1,-1):\n",
    "            var = le_row_years.iloc[curr_column_ix-ix-2]\n",
    "            x.append(var)\n",
    "\n",
    "        le_curr = le_row_years.iloc[curr_column_ix-1]\n",
    "        le_pred = le_row_years.iloc[curr_column_ix]\n",
    "\n",
    "        x.append(le_curr)\n",
    "        \n",
    "\n",
    "        x.append(country_code)\n",
    "        x.append(gdp)\n",
    "\n",
    "        y = [cp_pred, fr_pred, le_pred]\n",
    "        \n",
    "        cp_delta = cp_pred - cp_curr\n",
    "        fr_delta = fr_pred - fr_curr\n",
    "        le_delta = le_pred - le_curr\n",
    "\n",
    "        y_delta = [cp_delta, fr_delta, le_delta]\n",
    "\n",
    "        cp_roc = cp_pred / cp_curr\n",
    "        fr_roc = fr_pred / fr_curr\n",
    "        le_roc = le_pred / le_curr\n",
    "\n",
    "        y_roc = [cp_roc, fr_roc, le_roc]\n",
    "\n",
    "        X_list.append(x)\n",
    "        Y_list.append(y)\n",
    "        Y_delta_list.append(y_delta)\n",
    "        Y_roc_list.append(y_roc)\n",
    "\n",
    "Y_np = np.array(Y_list)\n",
    "Y_delta_np = np.array(Y_delta_list)\n",
    "Y_roc_np = np.array(Y_roc_list)\n",
    "X_np = np.array(X_list)\n",
    "\n",
    "assert len(Y_np) == len(X_np)\n",
    "\n",
    "X_df = pd.DataFrame(X_np, columns=X_labels)\n",
    "Y_df = pd.DataFrame(Y_np, columns=Y_labels)\n",
    "Y_delta_df = pd.DataFrame(Y_delta_np, columns=Y_delta_labels)\n",
    "Y_roc_df = pd.DataFrame(Y_roc_np, columns=Y_roc_labels)\n",
    "\n",
    "full_df = X_df.join([ Y_df, Y_delta_df, Y_roc_df ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Care of NaN Values and Assign Correct Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.dropna()\n",
    "full_df = shuffle(full_df, random_state = 13)\n",
    "\n",
    "nonobject_cols = [col \n",
    "for col in full_df \n",
    "    if full_df[col].apply(lambda x: \n",
    "            is_float(x)\n",
    "        ).all()\n",
    "    ]\n",
    "full_df[nonobject_cols] = full_df[nonobject_cols].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [col for col in X_df.columns if full_df[col].apply(lambda x: not isinstance(x, float)).all()]\n",
    "numerical_cols = [col for col in X_df.columns if full_df[col].apply(lambda x: isinstance(x, float)).any()]\n",
    "\n",
    "\n",
    "\n",
    "numerical_df = full_df[numerical_cols]\n",
    "categorical_df = full_df[categorical_cols]\n",
    "\n",
    "Y_df = full_df[Y_df.columns]\n",
    "Y_delta_df = full_df[Y_delta_df.columns]\n",
    "Y_roc_df = full_df[Y_roc_df.columns]\n",
    "\n",
    "\n",
    "# Change categorical features to int\n",
    "categorical_l = []\n",
    "for col in categorical_df.columns:\n",
    "    categorical_ixs = dict((y,x) for x,y in enumerate(np.unique(categorical_df[col].to_numpy())))\n",
    "    _categorical_l = [ \n",
    "        categorical_ixs[val] \n",
    "            for val in categorical_df[col].values\n",
    "        ]\n",
    "    categorical_l.append(_categorical_l)\n",
    "categorical_np = np.array(categorical_l)\n",
    "if len(categorical_l) > 1:\n",
    "    categorical_df = pd.DataFrame(categorical_np, columns=categorical_cols, dtype=int)\n",
    "else:\n",
    "    categorical_df = pd.DataFrame(categorical_np[0], columns=categorical_cols, dtype=int)\n",
    "\n",
    "# Merge full_df\n",
    "full_df = pd.concat([numerical_df,categorical_df, Y_df,Y_delta_df,Y_roc_df], axis=1).dropna()\n",
    "\n",
    "\n",
    "X_df = full_df[X_df.columns]\n",
    "Y_df = full_df[Y_df.columns]\n",
    "Y_delta_df = full_df[Y_delta_df.columns]\n",
    "Y_roc_df = full_df[Y_roc_df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_ratio = 0.90\n",
    "train_ratio = 0.80\n",
    "test_ratio = 0.15\n",
    "ivs_ratio = 0.05\n",
    "\n",
    "train_len = round(len(full_df) * train_ratio)\n",
    "test_len = round(len(full_df) * test_ratio)\n",
    "ivs_len = round(len(full_df) * ivs_ratio)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_df = X_df[0:train_len]\n",
    "X_train_np = X_train_df.to_numpy()\n",
    "\n",
    "X_test_df = X_df[train_len:train_len+test_len]\n",
    "X_test_np = X_test_df.to_numpy()\n",
    "\n",
    "X_ivs_df = X_df[train_len+test_len:train_len+test_len+ivs_len]\n",
    "X_ivs_np = X_ivs_df.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y_train_df = Y_df[0:train_len]\n",
    "Y_train_np = Y_train_df.to_numpy()\n",
    "\n",
    "Y_delta_train_df = Y_delta_df[0:train_len]\n",
    "Y_delta_train_np = Y_delta_train_df.to_numpy()\n",
    "\n",
    "Y_roc_train_df = Y_roc_df[0:train_len]\n",
    "Y_roc_train_np = Y_roc_train_df.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y_test_df = Y_df[train_len:train_len+test_len]\n",
    "Y_test_np = Y_test_df.to_numpy()\n",
    "\n",
    "Y_delta_test_df = Y_delta_df[train_len:train_len+test_len]\n",
    "Y_delta_test_np = Y_delta_test_df.to_numpy()\n",
    "\n",
    "Y_roc_test_df = Y_roc_df[train_len:train_len+test_len]\n",
    "Y_roc_test_np = Y_roc_test_df.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y_ivs_df = Y_df[train_len+test_len:train_len+test_len+ivs_len]\n",
    "Y_ivs_np = Y_ivs_df.to_numpy()\n",
    "\n",
    "Y_delta_ivs_df = Y_delta_df[train_len+test_len:train_len+test_len+ivs_len]\n",
    "Y_delta_ivs_np = Y_delta_ivs_df.to_numpy()\n",
    "\n",
    "Y_roc_ivs_df = Y_roc_df[train_len+test_len:train_len+test_len+ivs_len]\n",
    "Y_roc_ivs_np = Y_roc_ivs_df.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check what features are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SelectFromModel(estimator=RandomForestRegressor())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectFromModel</label><div class=\"sk-toggleable__content\"><pre>SelectFromModel(estimator=RandomForestRegressor())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "SelectFromModel(estimator=RandomForestRegressor())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False False False False False False False False]\n",
      "1\n",
      "Index(['country_population_curr'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "sel = SelectFromModel(RandomForestRegressor())\n",
    "estimator = sel.fit(X_train_df,Y_delta_train_df)\n",
    "display(estimator)\n",
    "\n",
    "\n",
    "print(sel.get_support())\n",
    "selected_feat = X_train_df.columns[(sel.get_support())]\n",
    "print(len(selected_feat))\n",
    "print(selected_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the random forest regressor used for feature selection only finds the current country population relevant for the model, but this isn't necessarely true, so this being a temporal problem, we disregard this obtained result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running tests using RandomForestRegressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RVE is:  0.9999523288066541\n",
      "The mse is:  3070055.635989374\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  49498152.25\n",
      "The Mean Absolute Error is:  468465.4885411233\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_train_np,Y_train_np)\n",
    "\n",
    "truth=Y_test_np\n",
    "\n",
    "preds=[]\n",
    "for x in X_test_np:\n",
    "    y = model.predict(x.reshape(1, -1))\n",
    "    preds.append(y)\n",
    "preds=np.array(preds)\n",
    "\n",
    "printRegStatistics(truth.flatten(),preds.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RVE is:  0.9990932019215302\n",
      "The mse is:  225386.08858804769\n",
      "The Correlation Score is is: 0.9995 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  4016066.3200000003\n",
      "The Mean Absolute Error is:  43733.54699546353\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_train_np,Y_delta_train_np)\n",
    "truth=Y_delta_test_np\n",
    "\n",
    "preds=[]\n",
    "for x in X_test_np:\n",
    "    y = model.predict(x.reshape(1, -1))\n",
    "    preds.append(y)\n",
    "preds=np.array(preds)\n",
    "\n",
    "printRegStatistics(truth.flatten(),preds.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Still using Delta) with raw results converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RVE is:  0.9999997369464351\n",
      "The mse is:  227914.3714548749\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  6274726.6000003815\n",
      "The Mean Absolute Error is:  43019.95216465159\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_train_np,Y_delta_train_np)\n",
    "truth=Y_test_np\n",
    "\n",
    "preds=[]\n",
    "curr_cp_ix = X_test_df.columns.to_list().index(\"country_population_curr\")\n",
    "curr_fr_ix = X_test_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "curr_le_ix = X_test_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "for x in X_test_np:\n",
    "    y = model.predict(x.reshape(1, -1))[0]\n",
    "    y += np.array([x[curr_cp_ix], x[curr_fr_ix], x[curr_le_ix]])\n",
    "    preds.append(y)\n",
    "preds=np.array(preds)\n",
    "\n",
    "printRegStatistics(truth.flatten(),preds.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Rate of Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RVE is:  0.672724419985684\n",
      "The mse is:  0.012226481775112279\n",
      "The Correlation Score is is: 0.8213 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  0.20886936559830382\n",
      "The Mean Absolute Error is:  0.004948772993100164\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_train_np,Y_roc_train_np)\n",
    "truth=Y_roc_test_np\n",
    "\n",
    "preds=[]\n",
    "for x in X_test_np:\n",
    "    y = model.predict(x.reshape(1, -1))\n",
    "    preds.append(list(y))\n",
    "preds=np.array(preds)\n",
    "\n",
    "printRegStatistics(truth.flatten(),preds.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Still using Rate of Change) with raw results converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RVE is:  0.9999990320653183\n",
      "The mse is:  437191.6736796969\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  8696255.400945663\n",
      "The Mean Absolute Error is:  75497.44784053067\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_train_np,Y_roc_train_np)\n",
    "truth=Y_test_np\n",
    "\n",
    "preds=[]\n",
    "curr_cp_ix = X_test_df.columns.to_list().index(\"country_population_curr\")\n",
    "curr_fr_ix = X_test_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "curr_le_ix = X_test_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "for x in X_test_np:\n",
    "    y = model.predict(x.reshape(1, -1))[0]\n",
    "    y = np.array([x[curr_cp_ix] * y[0], x[curr_fr_ix] * y[1], x[curr_le_ix] * y[2]])\n",
    "    preds.append(y)\n",
    "preds=np.array(preds)\n",
    "\n",
    "printRegStatistics(truth.flatten(),preds.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe, that using delta we can achieve the best results. So, we disregard raw predictions or rate of change usage and go with the Delta computation aproach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we separate the different features to predict and compute one individualy, to see if it fairs better\n",
    "\n",
    "We are only taking the best of the ones we tested for comparations, and trying out country population first.\n",
    "\n",
    "Thus, this test is for the use of the Delta computation to predict country population only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RVE is:  0.9999997417186166\n",
      "The mse is:  375342.06098282087\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  5014499.93999958\n",
      "The Mean Absolute Error is:  128932.92671885606\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_train_np,Y_delta_train_df[\"country_population_delta\"].to_numpy())\n",
    "truth=Y_test_df[\"country_population_pred\"].to_numpy()\n",
    "\n",
    "preds=[]\n",
    "curr_cp_ix = X_test_df.columns.to_list().index(\"country_population_curr\")\n",
    "curr_fr_ix = X_test_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "curr_le_ix = X_test_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "for x in X_test_np:\n",
    "    y = model.predict(x.reshape(1, -1))[0]\n",
    "    y += np.array([x[curr_cp_ix], x[curr_fr_ix], x[curr_le_ix]])\n",
    "    preds.append(y[0])\n",
    "preds=np.array(preds)\n",
    "\n",
    "printRegStatistics(truth,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we observe that it is of no use to try and separate each prediction feature, computing the batch actually can have better or equal results than each individualy! Which makes sense, when we remember the previously generated graphs, we could see that they had some correlation on the predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's optimize this and try different models\n",
    "\n",
    "### Chosing which models to optimize\n",
    "\n",
    "First, we use a general simple package (lazypredict) to test out a bunch of sickitlearn models at once. This is used only to select which models we want to try out to optimize.\n",
    "\n",
    "This package doesn't work well with batch predictions, so we use the country_population feature to predict. And even with this, some models will fail due to the non-scaled Y and incompatabilities with the desired value-range of predictions.\n",
    "\n",
    "This could also takes a while to compute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adjusted R-Squared</th>\n",
       "      <th>R-Squared</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>219095.65</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLarsCV</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>347054.76</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLarsIC</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>347054.76</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransformedTargetRegressor</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>347054.76</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>347054.76</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BayesianRidge</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>347114.26</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>370370.85</td>\n",
       "      <td>3.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RANSACRegressor</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>431050.73</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLars</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>443130.61</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>467744.83</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingRegressor</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>471813.57</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>525750.84</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeRegressor</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>538276.67</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lars</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>538569.52</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LarsCV</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>538569.52</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>551307.42</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HistGradientBoostingRegressor</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>575385.28</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrthogonalMatchingPursuitCV</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>621397.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>697541.69</td>\n",
       "      <td>1.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>956346.28</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeCV</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>996292.58</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianProcessRegressor</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1472185.34</td>\n",
       "      <td>3.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2129968.67</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2466877.13</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>3049254.85</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoCV</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>3113498.40</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDRegressor</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>3179189.33</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HuberRegressor</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>3397098.60</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrthogonalMatchingPursuit</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>3869228.20</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>4121963.94</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KernelRidge</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>4806751.23</td>\n",
       "      <td>1.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TweedieRegressor</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5037238.76</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveRegressor</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>7764860.81</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNetCV</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>12448338.61</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyRegressor</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>12451450.72</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVR</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>13004029.82</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuantileRegressor</th>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>13162656.49</td>\n",
       "      <td>4273.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>13162664.62</td>\n",
       "      <td>2.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>13188134.35</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVR</th>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>13210248.12</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Adjusted R-Squared  R-Squared        RMSE  \\\n",
       "Model                                                                      \n",
       "ExtraTreesRegressor                          1.00       1.00   219095.65   \n",
       "LassoLarsCV                                  1.00       1.00   347054.76   \n",
       "LassoLarsIC                                  1.00       1.00   347054.76   \n",
       "TransformedTargetRegressor                   1.00       1.00   347054.76   \n",
       "LinearRegression                             1.00       1.00   347054.76   \n",
       "BayesianRidge                                1.00       1.00   347114.26   \n",
       "RandomForestRegressor                        1.00       1.00   370370.85   \n",
       "RANSACRegressor                              1.00       1.00   431050.73   \n",
       "LassoLars                                    1.00       1.00   443130.61   \n",
       "XGBRegressor                                 1.00       1.00   467744.83   \n",
       "BaggingRegressor                             1.00       1.00   471813.57   \n",
       "DecisionTreeRegressor                        1.00       1.00   525750.84   \n",
       "ExtraTreeRegressor                           1.00       1.00   538276.67   \n",
       "Lars                                         1.00       1.00   538569.52   \n",
       "LarsCV                                       1.00       1.00   538569.52   \n",
       "LGBMRegressor                                1.00       1.00   551307.42   \n",
       "HistGradientBoostingRegressor                1.00       1.00   575385.28   \n",
       "OrthogonalMatchingPursuitCV                  1.00       1.00   621397.70   \n",
       "GradientBoostingRegressor                    1.00       1.00   697541.69   \n",
       "KNeighborsRegressor                          0.99       0.99   956346.28   \n",
       "RidgeCV                                      0.99       0.99   996292.58   \n",
       "GaussianProcessRegressor                     0.99       0.99  1472185.34   \n",
       "AdaBoostRegressor                            0.97       0.97  2129968.67   \n",
       "Ridge                                        0.96       0.96  2466877.13   \n",
       "Lasso                                        0.94       0.94  3049254.85   \n",
       "LassoCV                                      0.94       0.94  3113498.40   \n",
       "SGDRegressor                                 0.93       0.93  3179189.33   \n",
       "HuberRegressor                               0.92       0.93  3397098.60   \n",
       "OrthogonalMatchingPursuit                    0.90       0.90  3869228.20   \n",
       "ElasticNet                                   0.89       0.89  4121963.94   \n",
       "KernelRidge                                  0.85       0.85  4806751.23   \n",
       "TweedieRegressor                             0.83       0.84  5037238.76   \n",
       "PassiveAggressiveRegressor                   0.61       0.61  7764860.81   \n",
       "ElasticNetCV                                -0.01      -0.00 12448338.61   \n",
       "DummyRegressor                              -0.01      -0.00 12451450.72   \n",
       "NuSVR                                       -0.10      -0.09 13004029.82   \n",
       "QuantileRegressor                           -0.13      -0.12 13162656.49   \n",
       "SVR                                         -0.13      -0.12 13162664.62   \n",
       "MLPRegressor                                -0.13      -0.12 13188134.35   \n",
       "LinearSVR                                   -0.14      -0.13 13210248.12   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "ExtraTreesRegressor                  1.56  \n",
       "LassoLarsCV                          0.04  \n",
       "LassoLarsIC                          0.02  \n",
       "TransformedTargetRegressor           0.01  \n",
       "LinearRegression                     0.01  \n",
       "BayesianRidge                        0.04  \n",
       "RandomForestRegressor                3.92  \n",
       "RANSACRegressor                      0.03  \n",
       "LassoLars                            0.01  \n",
       "XGBRegressor                         0.24  \n",
       "BaggingRegressor                     0.41  \n",
       "DecisionTreeRegressor                0.07  \n",
       "ExtraTreeRegressor                   0.02  \n",
       "Lars                                 0.01  \n",
       "LarsCV                               0.02  \n",
       "LGBMRegressor                        0.12  \n",
       "HistGradientBoostingRegressor        0.64  \n",
       "OrthogonalMatchingPursuitCV          0.02  \n",
       "GradientBoostingRegressor            1.55  \n",
       "KNeighborsRegressor                  0.03  \n",
       "RidgeCV                              0.01  \n",
       "GaussianProcessRegressor             3.92  \n",
       "AdaBoostRegressor                    0.43  \n",
       "Ridge                                0.01  \n",
       "Lasso                                0.49  \n",
       "LassoCV                              0.46  \n",
       "SGDRegressor                         0.01  \n",
       "HuberRegressor                       0.10  \n",
       "OrthogonalMatchingPursuit            0.01  \n",
       "ElasticNet                           0.03  \n",
       "KernelRidge                          1.55  \n",
       "TweedieRegressor                     0.02  \n",
       "PassiveAggressiveRegressor           0.22  \n",
       "ElasticNetCV                         0.08  \n",
       "DummyRegressor                       0.01  \n",
       "NuSVR                                1.45  \n",
       "QuantileRegressor                 4273.58  \n",
       "SVR                                  2.13  \n",
       "MLPRegressor                         3.57  \n",
       "LinearSVR                            0.01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"./lazypredict_models.dump\", \"rb\") as o:\n",
    "    models = pickle.load(o)\n",
    "\n",
    "display(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this table, we set our plan to optimize a LinearRegression, a SVR and a RandomForestsRegressor Model. Finaly, we will try and get the best result possible with ExtraTreesRegressor, even though it will be slower, like the RandomForestsRegressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearRegression\n",
    "## Simply running the Model without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RVE is:  0.9999997966926134\n",
      "The mse is:  200372.15703502297\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  5434967.502389073\n",
      "The Mean Absolute Error is:  37186.270066838326\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train_np,Y_delta_train_np)\n",
    "truth=Y_test_np\n",
    "\n",
    "preds=[]\n",
    "curr_cp_ix = X_test_df.columns.to_list().index(\"country_population_curr\")\n",
    "curr_fr_ix = X_test_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "curr_le_ix = X_test_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "for x in X_test_np:\n",
    "    y = model.predict(x.reshape(1, -1))[0]\n",
    "    y += np.array([x[curr_cp_ix], x[curr_fr_ix], x[curr_le_ix]])\n",
    "    preds.append(y)\n",
    "preds=np.array(preds)\n",
    "\n",
    "printRegStatistics(truth.flatten(),preds.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With K Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Linear Regressor Results\n",
      "\n",
      "\n",
      "For the country population\n",
      "\n",
      "The RVE is:  0.9999998082694822\n",
      "The mse is:  329387.92678138695\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  5898102.585692406\n",
      "The Mean Absolute Error is:  86338.19756183107\n",
      "\n",
      "\n",
      "For the fertility rate\n",
      "\n",
      "The RVE is:  0.9995849998909416\n",
      "The mse is:  0.0392008771047105\n",
      "The Correlation Score is is: 0.9998 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  0.421196132695127\n",
      "The Mean Absolute Error is:  0.01957113113905548\n",
      "\n",
      "\n",
      "For the life expectancy\n",
      "\n",
      "The RVE is:  0.9997100207975071\n",
      "The mse is:  0.1776551402871918\n",
      "The Correlation Score is is: 0.9999 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  1.8786818075138356\n",
      "The Mean Absolute Error is:  0.08974679672894503\n",
      "\n",
      "\n",
      "Total Flattened\n",
      "\n",
      "The RVE is:  0.9999998222321135\n",
      "The mse is:  190172.20819507548\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  5898102.585692406\n",
      "The Mean Absolute Error is:  28779.43562658631\n",
      "\n",
      "\n",
      "Total\n",
      "\n",
      "The RVE is:  0.9999844256987634\n",
      "The mse is:  114233412.21585716\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  1113991954.2149467\n",
      "The Mean Absolute Error is:  36130571.16479758\n",
      "\n",
      "\n",
      "With hyperparams: fit_intercept=False; normalize=False; copy_X=False; n_jobs=60; positive=False.\n",
      "No other hyperparam combinations.\n"
     ]
    }
   ],
   "source": [
    "#fit_intercept = [True]\n",
    "fit_intercept = [True,False]\n",
    "#normalize = [\"deprecated\"]\n",
    "normalize = [True, False]\n",
    "#copy_X = [True]\n",
    "copy_X = [True, False]\n",
    "#n_jobs = [None]\n",
    "n_jobs = [None,1,30,50,55,60]\n",
    "#positive = [False]\n",
    "positive = [True,False]\n",
    "\n",
    "hyperparams = list(product(fit_intercept, normalize, copy_X, n_jobs, positive))\n",
    "best_model, best_hyperparams_possib, best_score = kfold_val(SupportedModelType.LINEAR_REGRESSION, [\"mse\"], X_train_df, X_test_df, pd.concat([Y_delta_train_df, Y_delta_test_df], axis=0), pd.concat([Y_train_df, Y_test_df], axis=0), hyperparams)\n",
    "best_hyperparams = best_hyperparams_possib[0]\n",
    "\n",
    "truth=Y_ivs_np\n",
    "\n",
    "preds=[]\n",
    "curr_cp_ix = X_ivs_df.columns.to_list().index(\"country_population_curr\")\n",
    "curr_fr_ix = X_ivs_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "curr_le_ix = X_ivs_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "for x in X_ivs_np:\n",
    "    y = best_model.predict(x.reshape(1, -1))[0]\n",
    "    y += np.array([x[curr_cp_ix], x[curr_fr_ix], x[curr_le_ix]])\n",
    "    preds.append(y)\n",
    "preds=np.array(preds)\n",
    "\n",
    "print()\n",
    "print(\"Best Linear Regressor Results\")\n",
    "\n",
    "truth_total = []\n",
    "truth_cp = []\n",
    "truth_fr = []\n",
    "truth_le = []\n",
    "for t in truth:\n",
    "    truth_total.append((t[0] * t[1] * t[2]) / 5)\n",
    "    truth_cp.append(t[0])\n",
    "    truth_fr.append(t[1])\n",
    "    truth_le.append(t[2])\n",
    "truth_total = np.array(truth_total)\n",
    "truth_cp = np.array(truth_cp)\n",
    "truth_fr = np.array(truth_fr)\n",
    "truth_le = np.array(truth_le)\n",
    "\n",
    "preds_total = []\n",
    "preds_cp = []\n",
    "preds_fr = []\n",
    "preds_le = []\n",
    "for p in preds:\n",
    "    preds_total.append((p[0] * p[1] * p[2]) / 5)\n",
    "    preds_cp.append(p[0])\n",
    "    preds_fr.append(p[1])\n",
    "    preds_le.append(p[2])\n",
    "preds_total = np.array(preds_total)\n",
    "preds_cp = np.array(preds_cp)\n",
    "preds_fr = np.array(preds_fr)\n",
    "preds_le = np.array(preds_le)\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"For the country population\")\n",
    "print()\n",
    "printRegStatistics(truth_cp,preds_cp)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"For the fertility rate\")\n",
    "print()\n",
    "printRegStatistics(truth_fr,preds_fr)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"For the life expectancy\")\n",
    "print()\n",
    "printRegStatistics(truth_le,preds_le)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Total Flattened\")\n",
    "print()\n",
    "printRegStatistics(truth.flatten(),preds.flatten())\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Total\")\n",
    "print()\n",
    "printRegStatistics(truth_total,preds_total)\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "fit_intercept,\\\n",
    "    normalize,\\\n",
    "    copy_X,\\\n",
    "    n_jobs,\\\n",
    "    positive = best_hyperparams\n",
    "print(f\"With hyperparams: fit_intercept={fit_intercept}; normalize={normalize}; copy_X={copy_X}; n_jobs={n_jobs}; positive={positive}.\")\n",
    "if len(best_hyperparams_possib) == 1:\n",
    "    print(\"No other hyperparam combinations.\")\n",
    "else:\n",
    "    print(f\"{str(len(best_hyperparams_possib) - 1)} other hyperparams with the same scores:\")\n",
    "    for p in best_hyperparams_possib:\n",
    "        fit_intercept,\\\n",
    "            normalize,\\\n",
    "            copy_X,\\\n",
    "            n_jobs,\\\n",
    "            positive = p\n",
    "        print(f\"With hyperparams: fit_intercept={fit_intercept}; normalize={normalize}; copy_X={copy_X}; n_jobs={n_jobs}; positive={positive}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR\n",
    "## Simply running the Model without optimization\n",
    "### With PCA Kernel for dimentionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RVE is:  0.9997178629910263\n",
      "The mse is:  7599482.111291332\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  82426878.54295969\n",
      "The Mean Absolute Error is:  1470333.9239080637\n"
     ]
    }
   ],
   "source": [
    "pca_kernel = KernelPCA()\n",
    "pca_kernel.fit(X_train_np,Y_delta_train_df[\"country_population_delta\"].to_numpy())\n",
    "\n",
    "pca_X_train_np = pca_kernel.transform(X_train_np)\n",
    "pca_X_test_np = [ pca_kernel.transform(x.reshape(1, -1)) for x in X_test_np ]\n",
    "\n",
    "# This one needs to be used for each prediction feature\n",
    "\n",
    "cp_model = SVR()\n",
    "\n",
    "cp_model.fit(pca_X_train_np,Y_delta_train_df[\"country_population_delta\"].to_numpy())\n",
    "\n",
    "fr_model = SVR()\n",
    "\n",
    "fr_model.fit(pca_X_train_np,Y_delta_train_df[\"fertility_rate_delta\"].to_numpy())\n",
    "\n",
    "le_model = SVR()\n",
    "le_model.fit(pca_X_train_np,Y_delta_train_df[\"life_expectancy_delta\"].to_numpy())\n",
    "\n",
    "truth=Y_test_np\n",
    "\n",
    "preds=[]\n",
    "curr_cp_ix = X_test_df.columns.to_list().index(\"country_population_curr\")\n",
    "curr_fr_ix = X_test_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "curr_le_ix = X_test_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "for ix in range(len(pca_X_test_np)):\n",
    "\n",
    "    x = pca_X_test_np[ix]\n",
    "\n",
    "    cp_y = cp_model.predict(x.reshape(1, -1))[0]\n",
    "    cp_y += X_test_np[ix][curr_cp_ix]\n",
    "    \n",
    "    fr_y = fr_model.predict(x.reshape(1, -1))[0]\n",
    "    fr_y += X_test_np[ix][curr_fr_ix]\n",
    "    \n",
    "    le_y = le_model.predict(x.reshape(1, -1))[0]\n",
    "    le_y += X_test_np[ix][curr_le_ix]\n",
    "\n",
    "    preds.append(np.array([ cp_y, fr_y, le_y ]))\n",
    "preds=np.array(preds)\n",
    "\n",
    "printRegStatistics(truth.flatten(),preds.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without PCA Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVR Results\n",
      "\n",
      "\n",
      "For the country population\n",
      "\n",
      "The RVE is:  0.9997158594505441\n",
      "The mse is:  13162691.147930166\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  82426894.80390263\n",
      "The Mean Absolute Error is:  4411002.638486993\n",
      "\n",
      "\n",
      "For the fertility rate\n",
      "\n",
      "The RVE is:  0.9987148968636281\n",
      "The mse is:  0.07138562313724392\n",
      "The Correlation Score is is: 0.9994 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  0.4328033571551808\n",
      "The Mean Absolute Error is:  0.05229561545246112\n",
      "\n",
      "\n",
      "For the life expectancy\n",
      "\n",
      "The RVE is:  0.9991042130309291\n",
      "The mse is:  0.3328081647534648\n",
      "The Correlation Score is is: 0.9996 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  3.622542837705211\n",
      "The Mean Absolute Error is:  0.21586857890565528\n",
      "\n",
      "\n",
      "Total Flattened\n",
      "\n",
      "The RVE is:  0.9997178629051118\n",
      "The mse is:  7599483.277517388\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  82426894.80390263\n",
      "The Mean Absolute Error is:  1470334.3022170623\n",
      "\n",
      "\n",
      "Total\n",
      "\n",
      "The RVE is:  0.9996218720289838\n",
      "The mse is:  639481340.011289\n",
      "The Correlation Score is is: 0.9999 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  6613668556.287201\n",
      "The Mean Absolute Error is:  215681864.42249528\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this one needs to be used for each prediction feature\n",
    "\n",
    "cp_model = SVR()\n",
    "cp_model.fit(X_train_np,Y_delta_train_df[\"country_population_delta\"].to_numpy())\n",
    "\n",
    "fr_model = SVR()\n",
    "fr_model.fit(X_train_np,Y_delta_train_df[\"fertility_rate_delta\"].to_numpy())\n",
    "\n",
    "le_model = SVR()\n",
    "le_model.fit(X_train_np,Y_delta_train_df[\"life_expectancy_delta\"].to_numpy())\n",
    "\n",
    "truth=Y_test_np\n",
    "\n",
    "preds=[]\n",
    "curr_cp_ix = X_test_df.columns.to_list().index(\"country_population_curr\")\n",
    "curr_fr_ix = X_test_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "curr_le_ix = X_test_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "for x in X_test_np:\n",
    "    cp_y = cp_model.predict(x.reshape(1, -1))[0]\n",
    "    cp_y += x[curr_cp_ix]\n",
    "    \n",
    "    fr_y = fr_model.predict(x.reshape(1, -1))[0]\n",
    "    fr_y += x[curr_fr_ix]\n",
    "    \n",
    "    le_y = le_model.predict(x.reshape(1, -1))[0]\n",
    "    le_y += x[curr_le_ix]\n",
    "\n",
    "    preds.append(np.array([ cp_y, fr_y, le_y ]))\n",
    "preds=np.array(preds)\n",
    "print()\n",
    "print(\"SVR Results\")\n",
    "\n",
    "preds_cp = []\n",
    "preds_fr = []\n",
    "preds_le = []\n",
    "preds_total = []\n",
    "for p in preds:\n",
    "    preds_total.append((p[0] * p[1] * p[2]) / 5)\n",
    "    preds_cp.append(p[0])\n",
    "    preds_fr.append(p[1])\n",
    "    preds_le.append(p[2])\n",
    "preds_cp = np.array(preds_cp)\n",
    "preds_fr = np.array(preds_fr)\n",
    "preds_le = np.array(preds_le)\n",
    "preds_total = np.array(preds_total)\n",
    "\n",
    "\n",
    "truth_total = []\n",
    "truth_cp = []\n",
    "truth_fr = []\n",
    "truth_le = []\n",
    "for t in truth:\n",
    "    truth_total.append((t[0] * t[1] * t[2]) / 5)\n",
    "    truth_cp.append(t[0])\n",
    "    truth_fr.append(t[1])\n",
    "    truth_le.append(t[2])\n",
    "truth_total = np.array(truth_total)\n",
    "truth_cp = np.array(truth_cp)\n",
    "truth_fr = np.array(truth_fr)\n",
    "truth_le = np.array(truth_le)\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"For the country population\")\n",
    "print()\n",
    "printRegStatistics(truth_cp,preds_cp)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"For the fertility rate\")\n",
    "print()\n",
    "printRegStatistics(truth_fr,preds_fr)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"For the life expectancy\")\n",
    "print()\n",
    "printRegStatistics(truth_le,preds_le)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Total Flattened\")\n",
    "print()\n",
    "printRegStatistics(truth.flatten(),preds.flatten())\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Total\")\n",
    "print()\n",
    "printRegStatistics(truth_total,preds_total)\n",
    "\n",
    "\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply kfold validation without the use of the pca kernel, as the results were identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With K Fold Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestsRegressor\n",
    "## Simply running the Model without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RVE is:  0.999999740872327\n",
      "The mse is:  226208.22186266698\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  6618293.960000038\n",
      "The Mean Absolute Error is:  43575.366938861036\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_train_np,Y_delta_train_np)\n",
    "truth=Y_test_np\n",
    "\n",
    "preds=[]\n",
    "curr_cp_ix = X_test_df.columns.to_list().index(\"country_population_curr\")\n",
    "curr_fr_ix = X_test_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "curr_le_ix = X_test_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "for x in X_test_np:\n",
    "    y = model.predict(x.reshape(1, -1))[0]\n",
    "    y += np.array([x[curr_cp_ix], x[curr_fr_ix], x[curr_le_ix]])\n",
    "    preds.append(y)\n",
    "preds=np.array(preds)\n",
    "\n",
    "printRegStatistics(truth.flatten(),preds.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With K Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best score: 1.0249687520306523e+18\n",
      "new best score: 7.095840290187991e+17\n",
      "score: 9.24117266984474e+17\n",
      "score: 6.557381964583295e+17\n",
      "\n",
      "Best Random Forest Regressor Results\n",
      "\n",
      "\n",
      "For the country population\n",
      "\n",
      "The RVE is:  0.9999999367592421\n",
      "The mse is:  189012.21359089282\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  1873316.109999895\n",
      "The Mean Absolute Error is:  67946.93279591737\n",
      "\n",
      "\n",
      "For the fertility rate\n",
      "\n",
      "The RVE is:  0.9995229886744046\n",
      "The mse is:  0.04206940948907107\n",
      "The Correlation Score is is: 0.9998 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  0.36360999999999954\n",
      "The Mean Absolute Error is:  0.021383205685112767\n",
      "\n",
      "\n",
      "For the life expectancy\n",
      "\n",
      "The RVE is:  0.9996783482220076\n",
      "The mse is:  0.18714529585551026\n",
      "The Correlation Score is is: 0.9998 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  1.2571397560974589\n",
      "The Mean Absolute Error is:  0.10187123652001023\n",
      "\n",
      "\n",
      "Total Flattened\n",
      "\n",
      "The RVE is:  0.9999999414315112\n",
      "The mse is:  109126.25239688519\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  1873316.109999895\n",
      "The Mean Absolute Error is:  22649.01868345319\n",
      "\n",
      "\n",
      "Total\n",
      "\n",
      "The RVE is:  0.9999855161342714\n",
      "The mse is:  110110340.30430949\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  1395071282.0091553\n",
      "The Mean Absolute Error is:  32210424.58954482\n",
      "\n",
      "\n",
      "With hyperparams: n_estimators=10; criterion=squared_error; max_depth=None; min_samples_split=2; min_samples_leaf=1; min_weight_fraction_leaf=0; max_features=3; max_leaf_nodes=None; min_impurity_decrease=0; bootstrap=True; oob_score=False; n_jobs=None; random_state=None; verbose=0; warm_start=False; ccp_alpha=0; max_samples=None.\n",
      "No other hyperparam combinations.\n"
     ]
    }
   ],
   "source": [
    "#n_estimators = [100]\n",
    "#n_estimators = [50,70,100,120,150,180,200,250]\n",
    "#n_estimators = [40,45,50,55,60,100]\n",
    "#n_estimators = [35,36,37,38,39,40,41,42,43,44,45]\n",
    "#n_estimators = [30,31,35]\n",
    "n_estimators = [10,20,30]\n",
    "criterion = [\"squared_error\"]\n",
    "#criterion = [\"squared_error\", \"absolute_error\"]\n",
    "max_depth = [None]\n",
    "min_samples_split = [2]\n",
    "min_samples_leaf = [1]\n",
    "min_weight_fraction_leaf = [0]\n",
    "#max_features = [1]\n",
    "max_features = [1,3]\n",
    "max_leaf_nodes = [None]\n",
    "min_impurity_decrease = [0]\n",
    "bootstrap = [True]\n",
    "oob_score = [False]\n",
    "n_jobs = [None]\n",
    "random_state = [None]\n",
    "verbose = [0]\n",
    "warm_start = [False]\n",
    "ccp_alpha = [0]\n",
    "max_samples = [None]\n",
    "\n",
    "hyperparams = list(product(\n",
    "    n_estimators,\n",
    "    criterion,\n",
    "    max_depth,\n",
    "    min_samples_split,\n",
    "    min_samples_leaf,\n",
    "    min_weight_fraction_leaf,\n",
    "    max_features,\n",
    "    max_leaf_nodes,\n",
    "    min_impurity_decrease,\n",
    "    bootstrap,\n",
    "    oob_score,\n",
    "    n_jobs,\n",
    "    random_state,\n",
    "    verbose,\n",
    "    warm_start,\n",
    "    ccp_alpha,\n",
    "    max_samples\n",
    "))\n",
    "best_model, best_hyperparams_possib, best_score = kfold_val(SupportedModelType.RANDOM_FOREST_REGRESSOR, [\"mse\"], X_train_df, X_test_df, pd.concat([Y_delta_train_df, Y_delta_test_df], axis=0), pd.concat([Y_train_df, Y_test_df], axis=0), hyperparams)\n",
    "best_hyperparams = best_hyperparams_possib[0]\n",
    "\n",
    "truth=Y_ivs_np\n",
    "\n",
    "preds=[]\n",
    "curr_cp_ix = X_ivs_df.columns.to_list().index(\"country_population_curr\")\n",
    "curr_fr_ix = X_ivs_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "curr_le_ix = X_ivs_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "for x in X_ivs_np:\n",
    "    y = model.predict(x.reshape(1, -1))[0]\n",
    "    y += np.array([x[curr_cp_ix], x[curr_fr_ix], x[curr_le_ix]])\n",
    "    preds.append(y)\n",
    "preds=np.array(preds)\n",
    "\n",
    "print()\n",
    "print(\"Best Random Forest Regressor Results\")\n",
    "\n",
    "truth_total = []\n",
    "truth_cp = []\n",
    "truth_fr = []\n",
    "truth_le = []\n",
    "for t in truth:\n",
    "    truth_total.append((t[0] * t[1] * t[2]) / 5)\n",
    "    truth_cp.append(t[0])\n",
    "    truth_fr.append(t[1])\n",
    "    truth_le.append(t[2])\n",
    "truth_total = np.array(truth_total)\n",
    "truth_cp = np.array(truth_cp)\n",
    "truth_fr = np.array(truth_fr)\n",
    "truth_le = np.array(truth_le)\n",
    "\n",
    "preds_total = []\n",
    "preds_cp = []\n",
    "preds_fr = []\n",
    "preds_le = []\n",
    "for p in preds:\n",
    "    preds_total.append((p[0] * p[1] * p[2]) / 5)\n",
    "    preds_cp.append(p[0])\n",
    "    preds_fr.append(p[1])\n",
    "    preds_le.append(p[2])\n",
    "preds_total = np.array(preds_total)\n",
    "preds_cp = np.array(preds_cp)\n",
    "preds_fr = np.array(preds_fr)\n",
    "preds_le = np.array(preds_le)\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"For the country population\")\n",
    "print()\n",
    "printRegStatistics(truth_cp,preds_cp)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"For the fertility rate\")\n",
    "print()\n",
    "printRegStatistics(truth_fr,preds_fr)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"For the life expectancy\")\n",
    "print()\n",
    "printRegStatistics(truth_le,preds_le)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Total Flattened\")\n",
    "print()\n",
    "printRegStatistics(truth.flatten(),preds.flatten())\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Total\")\n",
    "print()\n",
    "printRegStatistics(truth_total,preds_total)\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "n_estimators,\\\n",
    "    criterion,\\\n",
    "    max_depth,\\\n",
    "    min_samples_split,\\\n",
    "    min_samples_leaf,\\\n",
    "    min_weight_fraction_leaf,\\\n",
    "    max_features,\\\n",
    "    max_leaf_nodes,\\\n",
    "    min_impurity_decrease,\\\n",
    "    bootstrap,\\\n",
    "    oob_score,\\\n",
    "    n_jobs,\\\n",
    "    random_state,\\\n",
    "    verbose,\\\n",
    "    warm_start,\\\n",
    "    ccp_alpha,\\\n",
    "    max_samples = best_hyperparams\n",
    "print(f\"With hyperparams: n_estimators={n_estimators}; criterion={criterion}; max_depth={max_depth}; min_samples_split={min_samples_split}; min_samples_leaf={min_samples_leaf}; \\\n",
    "min_weight_fraction_leaf={min_weight_fraction_leaf}; max_features={max_features}; max_leaf_nodes={max_leaf_nodes}; min_impurity_decrease={min_impurity_decrease}; bootstrap={bootstrap}; \\\n",
    "oob_score={oob_score}; n_jobs={n_jobs}; random_state={random_state}; verbose={verbose}; warm_start={warm_start}; ccp_alpha={ccp_alpha}; max_samples={max_samples}.\")\n",
    "if len(best_hyperparams_possib) == 1:\n",
    "    print(\"No other hyperparam combinations.\")\n",
    "else:\n",
    "    print(f\"{str(len(best_hyperparams_possib) - 1)} other hyperparams with the same scores:\")\n",
    "    for p in best_hyperparams_possib:\n",
    "        n_estimators,\\\n",
    "            criterion,\\\n",
    "            max_depth,\\\n",
    "            min_samples_split,\\\n",
    "            min_samples_leaf,\\\n",
    "            min_weight_fraction_leaf,\\\n",
    "            max_features,\\\n",
    "            max_leaf_nodes,\\\n",
    "            min_impurity_decrease,\\\n",
    "            bootstrap,\\\n",
    "            oob_score,\\\n",
    "            n_jobs,\\\n",
    "            random_state,\\\n",
    "            verbose,\\\n",
    "            warm_start,\\\n",
    "            ccp_alpha,\\\n",
    "            max_samples = p\n",
    "        print(f\"With hyperparams: n_estimators={n_estimators}; criterion={criterion}; max_depth={max_depth}; min_samples_split={min_samples_split}; min_samples_leaf={min_samples_leaf}; \\\n",
    "min_weight_fraction_leaf={min_weight_fraction_leaf}; max_features={max_features}; max_leaf_nodes={max_leaf_nodes}; min_impurity_decrease={min_impurity_decrease}; bootstrap={bootstrap}; \\\n",
    "oob_score={oob_score}; n_jobs={n_jobs}; random_state={random_state}; verbose={verbose}; warm_start={warm_start}; ccp_alpha={ccp_alpha}; max_samples={max_samples}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesRegressor\n",
    "## Simply running the Model without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RVE is:  0.9999999116493357\n",
      "The mse is:  132084.7381687505\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  1985976.0399999619\n",
      "The Mean Absolute Error is:  26730.663645192526\n"
     ]
    }
   ],
   "source": [
    "model = ExtraTreesRegressor()\n",
    "model.fit(X_train_np,Y_delta_train_np)\n",
    "truth=Y_test_np\n",
    "\n",
    "preds=[]\n",
    "curr_cp_ix = X_test_df.columns.to_list().index(\"country_population_curr\")\n",
    "curr_fr_ix = X_test_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "curr_le_ix = X_test_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "for x in X_test_np:\n",
    "    y = model.predict(x.reshape(1, -1))[0]\n",
    "    y += np.array([x[curr_cp_ix], x[curr_fr_ix], x[curr_le_ix]])\n",
    "    preds.append(y)\n",
    "preds=np.array(preds)\n",
    "\n",
    "printRegStatistics(truth.flatten(),preds.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With K Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best score: 1.0945893597264851e+18\n",
      "new best score: 7.45932097476632e+17\n",
      "score: 7.988097855371882e+17\n",
      "score: 6.367872146778386e+17\n",
      "score: 8.91209886553896e+17\n",
      "score: 6.370588791291288e+17\n",
      "\n",
      "Best Extra Trees Regressor Results\n",
      "\n",
      "\n",
      "For the country population\n",
      "\n",
      "The RVE is:  0.9999999367592421\n",
      "The mse is:  189012.21359089282\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  1873316.109999895\n",
      "The Mean Absolute Error is:  67946.93279591737\n",
      "\n",
      "\n",
      "For the fertility rate\n",
      "\n",
      "The RVE is:  0.9995229886744046\n",
      "The mse is:  0.04206940948907107\n",
      "The Correlation Score is is: 0.9998 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  0.36360999999999954\n",
      "The Mean Absolute Error is:  0.021383205685112767\n",
      "\n",
      "\n",
      "For the life expectancy\n",
      "\n",
      "The RVE is:  0.9996783482220076\n",
      "The mse is:  0.18714529585551026\n",
      "The Correlation Score is is: 0.9998 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  1.2571397560974589\n",
      "The Mean Absolute Error is:  0.10187123652001023\n",
      "\n",
      "\n",
      "Total Flattened\n",
      "\n",
      "The RVE is:  0.9999999414315112\n",
      "The mse is:  109126.25239688519\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  1873316.109999895\n",
      "The Mean Absolute Error is:  22649.01868345319\n",
      "\n",
      "\n",
      "Total\n",
      "\n",
      "The RVE is:  0.9999855161342714\n",
      "The mse is:  110110340.30430949\n",
      "The Correlation Score is is: 1.0000 (p-value=0.000000e+00)\n",
      "\n",
      "The Maximum Error is is:  1395071282.0091553\n",
      "The Mean Absolute Error is:  32210424.58954482\n",
      "\n",
      "\n",
      "With hyperparams: n_estimators=10; criterion=squared_error; max_depth=None; min_samples_split=2; min_samples_leaf=1; min_weight_fraction_leaf=0; max_features=3; max_leaf_nodes=None; min_impurity_decrease=0; bootstrap=True; oob_score=False; n_jobs=None; random_state=None; verbose=0; warm_start=False; ccp_alpha=0; max_samples=None.\n",
      "No other hyperparam combinations.\n"
     ]
    }
   ],
   "source": [
    "#n_estimators = [100]\n",
    "#n_estimators = [50,70,100,120,150,180,200,250]\n",
    "#n_estimators = [30,50,70,100,120,150]\n",
    "n_estimators = [10,20,30]\n",
    "criterion = [\"squared_error\"]\n",
    "#criterion = [\"squared_error\", \"absolute_error\"]\n",
    "max_depth = [None]\n",
    "min_samples_split = [2]\n",
    "min_samples_leaf = [1]\n",
    "min_weight_fraction_leaf = [0]\n",
    "#max_features = [1]\n",
    "max_features = [1,3]\n",
    "max_leaf_nodes = [None]\n",
    "min_impurity_decrease = [0]\n",
    "bootstrap = [True]\n",
    "oob_score = [False]\n",
    "n_jobs = [None]\n",
    "random_state = [None]\n",
    "verbose = [0]\n",
    "warm_start = [False]\n",
    "ccp_alpha = [0]\n",
    "max_samples = [None]\n",
    "\n",
    "RandomForestRegressor()\n",
    "\n",
    "hyperparams = list(product(\n",
    "    n_estimators,\n",
    "    criterion,\n",
    "    max_depth,\n",
    "    min_samples_split,\n",
    "    min_samples_leaf,\n",
    "    min_weight_fraction_leaf,\n",
    "    max_features,\n",
    "    max_leaf_nodes,\n",
    "    min_impurity_decrease,\n",
    "    bootstrap,\n",
    "    oob_score,\n",
    "    n_jobs,\n",
    "    random_state,\n",
    "    verbose,\n",
    "    warm_start,\n",
    "    ccp_alpha,\n",
    "    max_samples\n",
    "))\n",
    "best_model, best_hyperparams_possib, best_score = kfold_val(SupportedModelType.EXTRA_TREES_REGRESSOR, [\"mse\"], X_train_df, X_test_df, pd.concat([Y_delta_train_df, Y_delta_test_df], axis=0), pd.concat([Y_train_df, Y_test_df], axis=0), hyperparams)\n",
    "best_hyperparams = best_hyperparams_possib[0]\n",
    "\n",
    "truth=Y_ivs_np\n",
    "\n",
    "preds=[]\n",
    "curr_cp_ix = X_ivs_df.columns.to_list().index(\"country_population_curr\")\n",
    "curr_fr_ix = X_ivs_df.columns.to_list().index(\"fertility_rate_curr\")\n",
    "curr_le_ix = X_ivs_df.columns.to_list().index(\"life_expectancy_curr\")\n",
    "for x in X_ivs_np:\n",
    "    y = model.predict(x.reshape(1, -1))[0]\n",
    "    y += np.array([x[curr_cp_ix], x[curr_fr_ix], x[curr_le_ix]])\n",
    "    preds.append(y)\n",
    "preds=np.array(preds)\n",
    "\n",
    "print()\n",
    "print(\"Best Extra Trees Regressor Results\")\n",
    "\n",
    "truth_total = []\n",
    "truth_cp = []\n",
    "truth_fr = []\n",
    "truth_le = []\n",
    "for t in truth:\n",
    "    truth_total.append((t[0] * t[1] * t[2]) / 5)\n",
    "    truth_cp.append(t[0])\n",
    "    truth_fr.append(t[1])\n",
    "    truth_le.append(t[2])\n",
    "truth_total = np.array(truth_total)\n",
    "truth_cp = np.array(truth_cp)\n",
    "truth_fr = np.array(truth_fr)\n",
    "truth_le = np.array(truth_le)\n",
    "\n",
    "preds_total = []\n",
    "preds_cp = []\n",
    "preds_fr = []\n",
    "preds_le = []\n",
    "for p in preds:\n",
    "    preds_total.append((p[0] * p[1] * p[2]) / 5)\n",
    "    preds_cp.append(p[0])\n",
    "    preds_fr.append(p[1])\n",
    "    preds_le.append(p[2])\n",
    "preds_total = np.array(preds_total)\n",
    "preds_cp = np.array(preds_cp)\n",
    "preds_fr = np.array(preds_fr)\n",
    "preds_le = np.array(preds_le)\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"For the country population\")\n",
    "print()\n",
    "printRegStatistics(truth_cp,preds_cp)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"For the fertility rate\")\n",
    "print()\n",
    "printRegStatistics(truth_fr,preds_fr)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"For the life expectancy\")\n",
    "print()\n",
    "printRegStatistics(truth_le,preds_le)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Total Flattened\")\n",
    "print()\n",
    "printRegStatistics(truth.flatten(),preds.flatten())\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Total\")\n",
    "print()\n",
    "printRegStatistics(truth_total,preds_total)\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "n_estimators,\\\n",
    "    criterion,\\\n",
    "    max_depth,\\\n",
    "    min_samples_split,\\\n",
    "    min_samples_leaf,\\\n",
    "    min_weight_fraction_leaf,\\\n",
    "    max_features,\\\n",
    "    max_leaf_nodes,\\\n",
    "    min_impurity_decrease,\\\n",
    "    bootstrap,\\\n",
    "    oob_score,\\\n",
    "    n_jobs,\\\n",
    "    random_state,\\\n",
    "    verbose,\\\n",
    "    warm_start,\\\n",
    "    ccp_alpha,\\\n",
    "    max_samples = best_hyperparams\n",
    "print(f\"With hyperparams: n_estimators={n_estimators}; criterion={criterion}; max_depth={max_depth}; min_samples_split={min_samples_split}; min_samples_leaf={min_samples_leaf}; \\\n",
    "min_weight_fraction_leaf={min_weight_fraction_leaf}; max_features={max_features}; max_leaf_nodes={max_leaf_nodes}; min_impurity_decrease={min_impurity_decrease}; bootstrap={bootstrap}; \\\n",
    "oob_score={oob_score}; n_jobs={n_jobs}; random_state={random_state}; verbose={verbose}; warm_start={warm_start}; ccp_alpha={ccp_alpha}; max_samples={max_samples}.\")\n",
    "if len(best_hyperparams_possib) == 1:\n",
    "    print(\"No other hyperparam combinations.\")\n",
    "else:\n",
    "    print(f\"{str(len(best_hyperparams_possib) - 1)} other hyperparams with the same scores:\")\n",
    "    for p in best_hyperparams_possib:\n",
    "        n_estimators,\\\n",
    "            criterion,\\\n",
    "            max_depth,\\\n",
    "            min_samples_split,\\\n",
    "            min_samples_leaf,\\\n",
    "            min_weight_fraction_leaf,\\\n",
    "            max_features,\\\n",
    "            max_leaf_nodes,\\\n",
    "            min_impurity_decrease,\\\n",
    "            bootstrap,\\\n",
    "            oob_score,\\\n",
    "            n_jobs,\\\n",
    "            random_state,\\\n",
    "            verbose,\\\n",
    "            warm_start,\\\n",
    "            ccp_alpha,\\\n",
    "            max_samples = p\n",
    "        print(f\"With hyperparams: n_estimators={n_estimators}; criterion={criterion}; max_depth={max_depth}; min_samples_split={min_samples_split}; min_samples_leaf={min_samples_leaf}; \\\n",
    "min_weight_fraction_leaf={min_weight_fraction_leaf}; max_features={max_features}; max_leaf_nodes={max_leaf_nodes}; min_impurity_decrease={min_impurity_decrease}; bootstrap={bootstrap}; \\\n",
    "oob_score={oob_score}; n_jobs={n_jobs}; random_state={random_state}; verbose={verbose}; warm_start={warm_start}; ccp_alpha={ccp_alpha}; max_samples={max_samples}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for 2017 and 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression(copy_X=False, fit_intercept=False, n_jobs=60, normalize=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression(copy_X=False, fit_intercept=False, n_jobs=60, normalize=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression(copy_X=False, fit_intercept=False, n_jobs=60, normalize=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesRegressor(bootstrap=True, ccp_alpha=0, max_features=3,\n",
       "                    min_impurity_decrease=0, min_weight_fraction_leaf=0,\n",
       "                    n_estimators=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesRegressor</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesRegressor(bootstrap=True, ccp_alpha=0, max_features=3,\n",
       "                    min_impurity_decrease=0, min_weight_fraction_leaf=0,\n",
       "                    n_estimators=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ExtraTreesRegressor(bootstrap=True, ccp_alpha=0, max_features=3,\n",
       "                    min_impurity_decrease=0, min_weight_fraction_leaf=0,\n",
       "                    n_estimators=10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_model = LinearRegression(fit_intercept=False, normalize=False, copy_X=False, n_jobs=60, positive=False)\n",
    "etr_model = ExtraTreesRegressor(n_estimators=10, criterion=\"squared_error\", max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0, max_features=3, max_leaf_nodes=None, min_impurity_decrease=0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0, max_samples=None)\n",
    "\n",
    "lr_model.fit(X_df.to_numpy(), Y_delta_df.to_numpy())\n",
    "etr_model.fit(X_df.to_numpy(), Y_delta_df.to_numpy())\n",
    "\n",
    "display(lr_model)\n",
    "display(etr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the random countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['United Kingdom',\n",
       " 'Barbados',\n",
       " 'South Africa',\n",
       " 'Ireland',\n",
       " 'Latin America & Caribbean',\n",
       " 'Europe & Central Asia',\n",
       " 'Bangladesh',\n",
       " 'Timor-Leste',\n",
       " 'Bahamas, The',\n",
       " 'East Asia & Pacific (IDA & IBRD countries)']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "countries_to_predict_code = [ random.choice(cp_df[cp_df[\"2016\"] != np.nan][\"Country Code\"].to_list()) for _ in range(0,10) ]\n",
    "countries_to_predict_name = []\n",
    "for _c in countries_to_predict_code:\n",
    "    c = cp_df[cp_df[\"Country Code\"] == _c][\"Country Name\"].values[0]\n",
    "    countries_to_predict_name.append(c)\n",
    "display(countries_to_predict_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cp_df = pd.read_csv(\"./imported_datasets/API_SP.POP.TOTL_DS2_en_csv_v2_4770387.csv\")\n",
    "new_fr_df = pd.read_csv(\"./imported_datasets/API_SP.DYN.TFRT.IN_DS2_en_csv_v2_4770506.csv\")\n",
    "new_le_df = pd.read_csv(\"./imported_datasets/API_SP.DYN.LE00.IN_DS2_en_csv_v2_4770434.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country of 'United Kingdom'\n",
      "Predicted Country Population for 2017: 65970067.87075951\n",
      "Real Country Population predicted for 2017: 66058859.0\n",
      "Predicted Fertility Rate for 2017: 1.8033641390693276\n",
      "Real Fertility Rate predicted for 2017: 1.74\n",
      "Predicted Life Expectancy for 2017: 81.02604606497155\n",
      "Real Life Expectancy predicted for 2017: 81.2560975609756\n",
      "\n",
      "Predicted Country Population for 2018: 66288088.73973277\n",
      "Real Country Population predicted for 2018: 66460344.0\n",
      "Predicted Fertility Rate for 2018: 1.8040188120922431\n",
      "Real Fertility Rate predicted for 2018: 1.68\n",
      "Predicted Life Expectancy for 2018: 81.13519512313846\n",
      "Real Life Expectancy predicted for 2018: 81.2560975609756\n",
      "\n",
      "Country of 'Barbados'\n",
      "Predicted Country Population for 2017: 284309.21463869954\n",
      "Real Country Population predicted for 2017: 279187.0\n",
      "Predicted Fertility Rate for 2017: 1.793071685938904\n",
      "Real Fertility Rate predicted for 2017: 1.615\n",
      "Predicted Life Expectancy for 2017: 76.05752783741171\n",
      "Real Life Expectancy predicted for 2017: 76.936\n",
      "\n",
      "Predicted Country Population for 2018: 281907.74849453213\n",
      "Real Country Population predicted for 2018: 279688.0\n",
      "Predicted Fertility Rate for 2018: 1.7855405759031686\n",
      "Real Fertility Rate predicted for 2018: 1.619\n",
      "Predicted Life Expectancy for 2018: 76.21113000029054\n",
      "Real Life Expectancy predicted for 2018: 77.067\n",
      "\n",
      "Country of 'South Africa'\n",
      "Predicted Country Population for 2017: 56678555.567617714\n",
      "Real Country Population predicted for 2017: 56641209.0\n",
      "Predicted Fertility Rate for 2017: 2.413102731983548\n",
      "Real Fertility Rate predicted for 2017: 2.334\n",
      "Predicted Life Expectancy for 2017: 63.360214250529275\n",
      "Real Life Expectancy predicted for 2017: 65.402\n",
      "\n",
      "Predicted Country Population for 2018: 57372521.35831925\n",
      "Real Country Population predicted for 2018: 57339635.0\n",
      "Predicted Fertility Rate for 2018: 2.356865062188927\n",
      "Real Fertility Rate predicted for 2018: 2.418\n",
      "Predicted Life Expectancy for 2018: 63.94101804262237\n",
      "Real Life Expectancy predicted for 2018: 65.674\n",
      "\n",
      "Country of 'Ireland'\n",
      "Predicted Country Population for 2017: 4864644.713164412\n",
      "Real Country Population predicted for 2017: 4807388.0\n",
      "Predicted Fertility Rate for 2017: 1.9086956472129804\n",
      "Real Fertility Rate predicted for 2017: 1.77\n",
      "Predicted Life Expectancy for 2017: 81.77441899437659\n",
      "Real Life Expectancy predicted for 2017: 82.1560975609756\n",
      "\n",
      "Predicted Country Population for 2018: 4963364.9396418575\n",
      "Real Country Population predicted for 2018: 4867316.0\n",
      "Predicted Fertility Rate for 2018: 1.8920072160285881\n",
      "Real Fertility Rate predicted for 2018: 1.75\n",
      "Predicted Life Expectancy for 2018: 81.95043621772413\n",
      "Real Life Expectancy predicted for 2018: 82.2048780487805\n",
      "\n",
      "Country of 'Latin America & Caribbean'\n",
      "Predicted Country Population for 2017: 643895449.3248692\n",
      "Real Country Population predicted for 2017: 633797190.0\n",
      "Predicted Fertility Rate for 2017: 2.0396799340646705\n",
      "Real Fertility Rate predicted for 2017: 1.9991667466459\n",
      "Predicted Life Expectancy for 2017: 75.75356817496439\n",
      "Real Life Expectancy predicted for 2017: 74.734493753292\n",
      "\n",
      "Predicted Country Population for 2018: 649415217.5091023\n",
      "Real Country Population predicted for 2018: 639628226.0\n",
      "Predicted Fertility Rate for 2018: 2.016173668036761\n",
      "Real Fertility Rate predicted for 2018: 1.97288326835601\n",
      "Predicted Life Expectancy for 2018: 75.9657554625069\n",
      "Real Life Expectancy predicted for 2018: 74.8518735371361\n",
      "\n",
      "Country of 'Europe & Central Asia'\n",
      "Predicted Country Population for 2017: 916289219.2403781\n",
      "Real Country Population predicted for 2017: 915855416.0\n",
      "Predicted Fertility Rate for 2017: 1.7574665428603193\n",
      "Real Fertility Rate predicted for 2017: 1.73476710806067\n",
      "Predicted Life Expectancy for 2017: 77.42149840820143\n",
      "Real Life Expectancy predicted for 2017: 77.7786266109075\n",
      "\n",
      "Predicted Country Population for 2018: 920729360.814176\n",
      "Real Country Population predicted for 2018: 918782553.0\n",
      "Predicted Fertility Rate for 2018: 1.764100296347593\n",
      "Real Fertility Rate predicted for 2018: 1.71619122399371\n",
      "Predicted Life Expectancy for 2018: 77.5861825656431\n",
      "Real Life Expectancy predicted for 2018: 77.9114517370265\n",
      "\n",
      "Country of 'Bangladesh'\n",
      "Predicted Country Population for 2017: 164621635.80507913\n",
      "Real Country Population predicted for 2017: 161793964.0\n",
      "Predicted Fertility Rate for 2017: 2.067190284010244\n",
      "Real Fertility Rate predicted for 2017: 2.043\n",
      "Predicted Life Expectancy for 2017: 72.83209548668663\n",
      "Real Life Expectancy predicted for 2017: 71.788\n",
      "\n",
      "Predicted Country Population for 2018: 166262138.90180975\n",
      "Real Country Population predicted for 2018: 163683958.0\n",
      "Predicted Fertility Rate for 2018: 2.0278847808509224\n",
      "Real Fertility Rate predicted for 2018: 2.04\n",
      "Predicted Life Expectancy for 2018: 73.19224851491104\n",
      "Real Life Expectancy predicted for 2018: 72.567\n",
      "\n",
      "Country of 'Timor-Leste'\n",
      "Predicted Country Population for 2017: 1298635.2985976376\n",
      "Real Country Population predicted for 2017: 1243235.0\n",
      "Predicted Fertility Rate for 2017: 5.3853903749267875\n",
      "Real Fertility Rate predicted for 2017: 3.596\n",
      "Predicted Life Expectancy for 2017: 69.19243217732227\n",
      "Real Life Expectancy predicted for 2017: 67.745\n",
      "\n",
      "Predicted Country Population for 2018: 1330271.5633119515\n",
      "Real Country Population predicted for 2018: 1261845.0\n",
      "Predicted Fertility Rate for 2018: 5.266316307243793\n",
      "Real Fertility Rate predicted for 2018: 3.452\n",
      "Predicted Life Expectancy for 2018: 69.51145593270864\n",
      "Real Life Expectancy predicted for 2018: 68.017\n",
      "\n",
      "Country of 'Bahamas, The'\n",
      "Predicted Country Population for 2017: 390679.10746984306\n",
      "Real Country Population predicted for 2017: 399020.0\n",
      "Predicted Fertility Rate for 2017: 1.7645281623274398\n",
      "Real Fertility Rate predicted for 2017: 1.426\n",
      "Predicted Life Expectancy for 2017: 75.85398160824654\n",
      "Real Life Expectancy predicted for 2017: 73.632\n",
      "\n",
      "Predicted Country Population for 2018: 388405.7182364271\n",
      "Real Country Population predicted for 2018: 401906.0\n",
      "Predicted Fertility Rate for 2018: 1.7635561737898655\n",
      "Real Fertility Rate predicted for 2018: 1.412\n",
      "Predicted Life Expectancy for 2018: 76.04487496285837\n",
      "Real Life Expectancy predicted for 2018: 73.806\n",
      "\n",
      "Country of 'East Asia & Pacific (IDA & IBRD countries)'\n",
      "Predicted Country Population for 2017: 2042756978.9746826\n",
      "Real Country Population predicted for 2017: 2055414680.0\n",
      "Predicted Fertility Rate for 2017: 1.8516007100744927\n",
      "Real Fertility Rate predicted for 2017: 1.95382639771591\n",
      "Predicted Life Expectancy for 2017: 74.69695259911698\n",
      "Real Life Expectancy predicted for 2017: 75.4160599170928\n",
      "\n",
      "Predicted Country Population for 2018: 2057667003.0698383\n",
      "Real Country Population predicted for 2018: 2068898629.0\n",
      "Predicted Fertility Rate for 2018: 1.8566208497204317\n",
      "Real Fertility Rate predicted for 2018: 1.77672361925831\n",
      "Predicted Life Expectancy for 2018: 74.86910891447344\n",
      "Real Life Expectancy predicted for 2018: 75.836017901148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ix in range(len(countries_to_predict_code)):\n",
    "    country_code = countries_to_predict_code[ix]\n",
    "    country_name = countries_to_predict_name[ix]\n",
    "\n",
    "    cp_2016_data = cp_df[cp_df[\"Country Code\"] == country_code][\"2016\"].values[0]\n",
    "    fr_2016_data = fr_df[fr_df[\"Country Code\"] == country_code][\"2016\"].values[0]\n",
    "    le_2016_data = le_df[le_df[\"Country Code\"] == country_code][\"2016\"].values[0]\n",
    "    gdp_2016_data = gdp_df[gdp_df[\"Country Code\"] == country_code][\"2016\"].values[0]\n",
    "\n",
    "    \n",
    "    cp_2015_data = cp_df[cp_df[\"Country Code\"] == country_code][\"2015\"].values[0]\n",
    "    fr_2015_data = fr_df[fr_df[\"Country Code\"] == country_code][\"2015\"].values[0]\n",
    "    le_2015_data = le_df[le_df[\"Country Code\"] == country_code][\"2015\"].values[0]\n",
    "    cp_2014_data = cp_df[cp_df[\"Country Code\"] == country_code][\"2014\"].values[0]\n",
    "    fr_2014_data = fr_df[fr_df[\"Country Code\"] == country_code][\"2014\"].values[0]\n",
    "    le_2014_data = le_df[le_df[\"Country Code\"] == country_code][\"2014\"].values[0]\n",
    "\n",
    "    print(f\"Country of '{country_name}'\")\n",
    "    #print(\"cp 2016: \" + str(cp_2016_data))\n",
    "    #print(\"fr 2016: \" + str(fr_2016_data))\n",
    "    #print(\"le 2016: \" + str(le_2016_data))\n",
    "    #print(\"gdp 2016: \" + str(gdp_2016_data))\n",
    "    \n",
    "    #print(\"cp 2015: \" + str(cp_2015_data))\n",
    "    #print(\"fr 2015: \" + str(fr_2015_data))\n",
    "    #print(\"le 2015: \" + str(le_2015_data))\n",
    "    \n",
    "    #print(\"cp 2014: \" + str(cp_2014_data))\n",
    "    #print(\"fr 2014: \" + str(fr_2014_data))\n",
    "    #print(\"le 2014: \" + str(le_2014_data))\n",
    "\n",
    "    # country_population_prev_1, country_population_prev_0, country_population_curr, fertility_rate_prev_1, fertility_rate_prev_0, fertility_rate_curr, life_expectancy_prev_1, life_expectancy_prev_0, life_expectancy_curr, country_code, gdp_curr\n",
    "    x = np.array([cp_2014_data, cp_2015_data, cp_2016_data, fr_2014_data, fr_2015_data, fr_2016_data, le_2014_data, le_2015_data, le_2016_data, categorical_ixs[country_code], gdp_2016_data])\n",
    "    lr_2017_preds = lr_model.predict(x.reshape(1, -1))[0]\n",
    "    etr_2017_preds = etr_model.predict(x.reshape(1, -1))[0]\n",
    "\n",
    "    #print(\"lr_preds_2017: \" + str(lr_2017_preds))\n",
    "    #print(\"etr_preds_2017: \" + str(etr_2017_preds))\n",
    "\n",
    "    gdp_2017_data = gdp_df[gdp_df[\"Country Code\"] == country_code][\"2017\"].values[0]\n",
    "    \n",
    "    #print(\"gdp 2017: \" + str(gdp_2017_data))\n",
    "\n",
    "    cp_2017_data = ((lr_2017_preds[0] + etr_2017_preds[0]) / 2) + cp_2016_data\n",
    "    fr_2017_data = ((lr_2017_preds[1] + etr_2017_preds[1]) / 2) + fr_2016_data\n",
    "    le_2017_data = ((lr_2017_preds[2] + etr_2017_preds[2]) / 2) + le_2016_data\n",
    "\n",
    "    x = np.array([cp_2015_data, cp_2016_data, cp_2017_data, fr_2015_data, fr_2016_data, fr_2017_data, le_2015_data, le_2016_data, le_2017_data, categorical_ixs[country_code], gdp_2017_data])\n",
    "    lr_2018_preds = lr_model.predict(x.reshape(1, -1))[0]\n",
    "    etr_2018_preds = etr_model.predict(x.reshape(1, -1))[0]\n",
    "\n",
    "    cp_2018_data = ((lr_2018_preds[0] + etr_2018_preds[0]) / 2) + cp_2017_data\n",
    "    fr_2018_data = ((lr_2018_preds[1] + etr_2018_preds[1]) / 2) + fr_2017_data\n",
    "    le_2018_data = ((lr_2018_preds[2] + etr_2018_preds[2]) / 2) + le_2017_data\n",
    "\n",
    "    print(f\"Predicted Country Population for 2017: {cp_2017_data}\")\n",
    "    real_cp_2017 = new_cp_df[new_cp_df[\"Country Code\"] == country_code][\"2017\"].values[0]\n",
    "    print(f\"Real Country Population predicted for 2017: {str(real_cp_2017)}\")\n",
    "    print(f\"Predicted Fertility Rate for 2017: {fr_2017_data}\")\n",
    "    real_fr_2017 = new_fr_df[new_fr_df[\"Country Code\"] == country_code][\"2017\"].values[0]\n",
    "    print(f\"Real Fertility Rate predicted for 2017: {real_fr_2017}\")\n",
    "    print(f\"Predicted Life Expectancy for 2017: {le_2017_data}\")\n",
    "    real_le_2017 = new_le_df[new_le_df[\"Country Code\"] == country_code][\"2017\"].values[0]\n",
    "    print(f\"Real Life Expectancy predicted for 2017: {real_le_2017}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(f\"Predicted Country Population for 2018: {cp_2018_data}\")\n",
    "    real_cp_2018 = new_cp_df[new_cp_df[\"Country Code\"] == country_code][\"2018\"].values[0]\n",
    "    print(f\"Real Country Population predicted for 2018: {real_cp_2018}\")\n",
    "    print(f\"Predicted Fertility Rate for 2018: {fr_2018_data}\")\n",
    "    real_fr_2018 = new_fr_df[new_fr_df[\"Country Code\"] == country_code][\"2018\"].values[0]\n",
    "    print(f\"Real Fertility Rate predicted for 2018: {real_fr_2018}\")\n",
    "    print(f\"Predicted Life Expectancy for 2018: {le_2018_data}\")\n",
    "    real_le_2018 = new_le_df[new_le_df[\"Country Code\"] == country_code][\"2018\"].values[0]\n",
    "    print(f\"Real Life Expectancy predicted for 2018: {real_le_2018}\")\n",
    "\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_ixs[\"USA\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "055142e5cf66ed8ee87e97d9d29944a505c8696ace08c4c3475747b47f85af0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
