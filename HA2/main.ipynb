{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, normalize\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix, accuracy_score\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "def printClassResults(truth, preds):\n",
    "    print(\"The Accuracy is: %7.4f\" % accuracy_score(truth, preds))\n",
    "    print(\"The Precision is: %7.4f\" % precision_score(truth, preds, pos_label='RB'))\n",
    "    print(\"The Recall is: %7.4f\" % recall_score(truth, preds, pos_label='RB'))\n",
    "    print(\"The F1 score is: %7.4f\" % f1_score(truth, preds, pos_label='RB'))\n",
    "    print(\"The Matthews correlation coefficient is: %7.4f\" % matthews_corrcoef(truth, preds))\n",
    "    print()\n",
    "    print(\"This is the Confusion Matrix\")\n",
    "    print(pd.DataFrame(confusion_matrix(truth, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "41\n",
      "     nHM F04 NssssC nCb nO F03 nN_N nArNO2 nCRX3 B01  ...       SpMax_A  \\\n",
      "0      0   0      0   2  2   0    0      0     0   0  ...  7.635193e-01   \n",
      "1      1   0      0   0  3   0    0      0     0   0  ... -4.481867e-02   \n",
      "2      0   0      0   2  2   0    0      0     0   0  ...  2.805893e-14   \n",
      "3      0   0      0   2  1   0    0      0     0   0  ...  6.022004e-01   \n",
      "4      0   0      0   0  0   0    0      0     0   0  ... -1.439928e+00   \n",
      "...   ..  ..    ...  .. ..  ..  ...    ...   ...  ..  ...           ...   \n",
      "4559   0   1      0   0  1   1    0      0     0   0  ...  2.805893e-14   \n",
      "4560   0   0      0   0  3   0    0      0     0   0  ... -6.781979e-01   \n",
      "4561   0   0      0   0  2   0    0      0     0   0  ...  1.261149e+00   \n",
      "4562   0   0      0   0  1   0    0      0     0   0  ... -8.396889e-01   \n",
      "4563   0   0      0   2  1   0    0      0     0   0  ...  2.624708e-01   \n",
      "\n",
      "      Psi_i_1d       SdO     TI2_L      nCrt   SpMax_B       Psi_i_A  \\\n",
      "0    -0.036040  0.083128 -0.584680 -0.100618  0.238243  2.500987e-01   \n",
      "1    -0.456402  1.084060 -0.169867 -0.100618  0.000000  3.805872e-15   \n",
      "2     0.156821  0.159556 -0.027209 -0.100618  0.106006  3.805872e-15   \n",
      "3    -0.106805 -0.983609 -0.757658 -0.100618  0.000000 -5.215966e-01   \n",
      "4     2.047832 -0.983609 -0.481798 -0.100618 -0.639572  6.946340e-02   \n",
      "...        ...       ...       ...       ...       ...           ...   \n",
      "4559  0.046197  0.120121  2.653844 -0.100618 -0.578582 -1.324588e+00   \n",
      "4560 -0.225453  0.075614 -0.083576 -0.100618 -0.346790  8.524926e-01   \n",
      "4561  0.046197  0.238106  2.790161 -0.100618 -0.197445 -5.752902e-01   \n",
      "4562  0.310610 -0.983609  0.158243 -0.100618  0.000000 -6.914044e-01   \n",
      "4563  0.345826 -0.983609 -0.770957 -0.100618  0.107317  3.805872e-15   \n",
      "\n",
      "         SM6_B            nX  Biodegradable  \n",
      "0     0.139808 -1.602862e-01             RB  \n",
      "1     1.289686  2.612766e-17             RB  \n",
      "2     0.321156 -1.602862e-01             RB  \n",
      "3    -0.044913 -1.602862e-01             RB  \n",
      "4    -0.955872 -1.602862e-01             RB  \n",
      "...        ...           ...            ...  \n",
      "4559  0.190793 -1.602862e-01            NRB  \n",
      "4560 -0.454077 -1.602862e-01             RB  \n",
      "4561  0.630875 -1.602862e-01             RB  \n",
      "4562 -0.948363  2.612766e-17             RB  \n",
      "4563 -0.145778 -1.602862e-01             RB  \n",
      "\n",
      "[4564 rows x 42 columns]\n",
      "(4564, 42)\n",
      "(889, 42)\n"
     ]
    }
   ],
   "source": [
    "initial_dataset = pd.read_csv(\"biodegradable_a.csv\").sample(frac=1).reset_index(drop=True)\n",
    "total_len, _ = initial_dataset.shape\n",
    "\n",
    "# NOTE - NO INDEPENDENT VALIDATION SET !!!\n",
    "\n",
    "# Total with means\n",
    "#categorical = ['int16', 'int32', 'int64']\n",
    "#biodegradable = ['object']\n",
    "#numerical = ['float16', 'float32', 'float64']\n",
    "\n",
    "class_cols = [col for col in initial_dataset.drop(\"Biodegradable\", axis=1) if initial_dataset[col].apply(lambda x: x % 1 == 0).all()]\n",
    "num_cols = [col for col in initial_dataset.drop(\"Biodegradable\", axis=1) if initial_dataset[col].apply(lambda x: x % 1 != 0).any()]\n",
    "\n",
    "print(len(class_cols) + len(num_cols))\n",
    "print(len(initial_dataset.drop(\"Biodegradable\", axis=1).columns))\n",
    "\n",
    "total_categorical_dataset = initial_dataset[class_cols]\n",
    "#total_categorical_dataset = total_categorical_dataset.fillna(total_categorical_dataset.mode())\n",
    "total_categorical_dataset = total_categorical_dataset.fillna(-1)\n",
    "total_categorical_dataset = total_categorical_dataset.astype(int).astype(object).astype(str)\n",
    "#print(total_categorical_dataset)\n",
    "\n",
    "total_numerical_dataset = initial_dataset[num_cols]\n",
    "total_numerical_dataset = total_numerical_dataset.fillna(total_numerical_dataset.mean())\n",
    "\n",
    "total_biodegradable = initial_dataset[\"Biodegradable\"]\n",
    "#total_biodegradable = initial_dataset.select_dtypes(include=biodegradable)\n",
    "#total_biodegradable = total_biodegradable.fillna(\"\")\n",
    "\n",
    "# Scale numerical data\n",
    "# https://scikit-learn.org/stable/modules/preproce\n",
    "#print(total_numerical_dataset)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Commenting the following two lines will run the models without scaling. It allows the usage of certain Naive Bayes models, but ruins SVMs.\n",
    "# As the NaiveBayes that don't work with the negative numbers that come from the scaling of data, we keep it for the ones that show better results\n",
    "total_numerical_dataset = pd.DataFrame(scaler.fit_transform(total_numerical_dataset),\n",
    "             columns=total_numerical_dataset.columns, index=total_numerical_dataset.index)\n",
    "\n",
    "\n",
    "#total_numerical_dataset = pd.DataFrame(normalize(total_numerical_dataset, norm='l2', axis=1, copy=True, return_norm=False),\n",
    "#             columns=total_numerical_dataset.columns, index=total_numerical_dataset.index)\n",
    "\n",
    "#print(total_numerical_dataset)\n",
    "#\n",
    "total_dataset = pd.concat([total_categorical_dataset, total_numerical_dataset,total_biodegradable], axis=1)\n",
    "#total_dataset.dropna(0)\n",
    "print(total_dataset)\n",
    "\n",
    "total_len, _ = total_dataset.shape\n",
    "train_dataset_len = round(total_len * 0.75)\n",
    "\n",
    "dataset_train = total_dataset[0:train_dataset_len]\n",
    "dataset_test = total_dataset[train_dataset_len:total_len]\n",
    "\n",
    "print(total_dataset.shape)\n",
    "\n",
    "# Removal of None/NaN vals\n",
    "dropna_dataset = initial_dataset.dropna()\n",
    "\n",
    "dropna_len, _ = dropna_dataset.shape\n",
    "model_dropna_len = round(total_len * 0.75)\n",
    "\n",
    "dropna_train = dropna_dataset[0:model_dropna_len]\n",
    "dropna_test = dropna_dataset[model_dropna_len:dropna_len]\n",
    "\n",
    "print(dropna_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['nHM', 'F04', 'NssssC', 'nCb', 'nO', 'F03', 'nN_N', 'nArNO2', 'nCRX3',\n",
      "       'B01', 'B03', 'N_073', 'B04', 'C_026', 'F02_CN', 'nHDon', 'nN',\n",
      "       'nArCOOR', 'SpMax_L', 'J_Dz(e)', 'F01', 'C', 'nCp', 'SdssC', 'HyWi_B',\n",
      "       'LOC', 'SM6_L', 'F03_CO', 'Me', 'Mi', 'SpPosA_B', 'nCIR', 'SpMax_A',\n",
      "       'Psi_i_1d', 'SdO', 'TI2_L', 'nCrt', 'SpMax_B', 'Psi_i_A', 'SM6_B', 'nX',\n",
      "       'Biodegradable'],\n",
      "      dtype='object')\n",
      "Index(['SpMax_L', 'J_Dz(e)', 'nHM', 'F01', 'F04', 'NssssC', 'nCb', 'C', 'nCp',\n",
      "       'nO', 'F03', 'SdssC', 'HyWi_B', 'LOC', 'SM6_L', 'F03_CO', 'Me', 'Mi',\n",
      "       'nN_N', 'nArNO2', 'nCRX3', 'SpPosA_B', 'nCIR', 'B01', 'B03', 'N_073',\n",
      "       'SpMax_A', 'Psi_i_1d', 'B04', 'SdO', 'TI2_L', 'nCrt', 'C_026', 'F02_CN',\n",
      "       'nHDon', 'SpMax_B', 'Psi_i_A', 'nN', 'SM6_B', 'nArCOOR', 'nX',\n",
      "       'Biodegradable'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(total_dataset.columns)\n",
    "print(dropna_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Model with replaced values when NaN, and discarding the dropped NaN values dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     nHM F04 NssssC nCb nO F03 nN_N nArNO2 nCRX3 B01  ...      nCIR  \\\n",
      "0      0   0      0   2  2   0    0      0     0   0  ...  0.117354   \n",
      "1      1   0      0   0  3   0    0      0     0   0  ... -0.302031   \n",
      "2      0   0      0   2  2   0    0      0     0   0  ...  0.536738   \n",
      "3      0   0      0   2  1   0    0      0     0   0  ...  0.117354   \n",
      "4      0   0      0   0  0   0    0      0     0   0  ... -0.302031   \n",
      "...   ..  ..    ...  .. ..  ..  ...    ...   ...  ..  ...       ...   \n",
      "3418   0   0      0   0  2   0    0      0     0   0  ... -0.302031   \n",
      "3419   0   0      0   2  2   0    0      0     0   0  ...  0.117354   \n",
      "3420   0   0      0   0  1   0    0      0     0   0  ... -0.302031   \n",
      "3421   0   0      0   0  2   0    0      0     0   0  ... -0.302031   \n",
      "3422   0   0      0   0  2   0    0      0     0   0  ... -0.302031   \n",
      "\n",
      "           SpMax_A  Psi_i_1d       SdO     TI2_L      nCrt   SpMax_B  \\\n",
      "0     7.635193e-01 -0.036040  0.083128 -0.584680 -0.100618  0.238243   \n",
      "1    -4.481867e-02 -0.456402  1.084060 -0.169867 -0.100618  0.000000   \n",
      "2     2.805893e-14  0.156821  0.159556 -0.027209 -0.100618  0.106006   \n",
      "3     6.022004e-01 -0.106805 -0.983609 -0.757658 -0.100618  0.000000   \n",
      "4    -1.439928e+00  2.047832 -0.983609 -0.481798 -0.100618 -0.639572   \n",
      "...            ...       ...       ...       ...       ...       ...   \n",
      "3418 -1.646051e+00  0.590609 -0.030898 -0.675984 -0.100618 -0.565504   \n",
      "3419  6.228061e-01  0.138396  0.088124 -0.535523 -0.100618  0.000000   \n",
      "3420 -6.379969e-01  0.384358 -0.983609 -0.155574 -0.100618 -1.104080   \n",
      "3421 -2.357405e-02  0.378499  1.055580 -0.249307 -0.100618  0.000000   \n",
      "3422 -2.834807e-01  0.559866  0.067613 -0.566726 -0.100618  0.000000   \n",
      "\n",
      "           Psi_i_A     SM6_B            nX  \n",
      "0     2.500987e-01  0.139808 -1.602862e-01  \n",
      "1     3.805872e-15  1.289686  2.612766e-17  \n",
      "2     3.805872e-15  0.321156 -1.602862e-01  \n",
      "3    -5.215966e-01 -0.044913 -1.602862e-01  \n",
      "4     6.946340e-02 -0.955872 -1.602862e-01  \n",
      "...            ...       ...           ...  \n",
      "3418  2.640763e+00 -0.992050 -1.602862e-01  \n",
      "3419  6.018873e-01  0.171440 -1.602862e-01  \n",
      "3420  3.805872e-15 -1.153264 -1.602862e-01  \n",
      "3421  3.805872e-15 -0.130313 -1.602862e-01  \n",
      "3422  6.037654e-01 -0.482184 -1.602862e-01  \n",
      "\n",
      "[3423 rows x 41 columns]\n",
      "0       RB\n",
      "1       RB\n",
      "2       RB\n",
      "3       RB\n",
      "4       RB\n",
      "        ..\n",
      "3418    RB\n",
      "3419    RB\n",
      "3420    RB\n",
      "3421    RB\n",
      "3422    RB\n",
      "Name: Biodegradable, Length: 3423, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X_train_total = dataset_train.drop([\"Biodegradable\"], axis=1)\n",
    "y_train_total = dataset_train.Biodegradable\n",
    "print(X_train_total)\n",
    "print(y_train_total)\n",
    "\n",
    "X_test_total = dataset_test.drop([\"Biodegradable\"], axis=1)\n",
    "y_test_total = dataset_test.Biodegradable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Random Forests for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SelectFromModel(estimator=RandomForestClassifier(), max_features=12)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectFromModel</label><div class=\"sk-toggleable__content\"><pre>SelectFromModel(estimator=RandomForestClassifier(), max_features=12)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(), max_features=12)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f\n",
    "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100), max_features=12) # when no max_features are specified, seems to vary between 12 and 15\n",
    "sel.fit(X_train_total, y_train_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True False  True False False False False False False\n",
      " False False  True False False False  True False False False False False\n",
      " False False  True False False False  True False False False False False\n",
      " False  True False  True  True]\n",
      "12\n",
      "Index(['nHM', 'F04', 'NssssC', 'nCb', 'F03', 'F02_CN', 'SpMax_L', 'SM6_L',\n",
      "       'SpPosA_B', 'SpMax_B', 'SM6_B', 'nX'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sel.get_support())\n",
    "selected_feat= X_train_total.columns[(sel.get_support())]\n",
    "print(len(selected_feat))\n",
    "print(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     nHM F04 NssssC nCb F03 F02_CN   SpMax_L     SM6_L  SpPosA_B   SpMax_B  \\\n",
      "0      0   0      0   2   0      0  0.502761  0.262102  0.467673  0.238243   \n",
      "1      1   0      0   0   0      4  1.064719  0.332217 -0.831613  0.000000   \n",
      "2      0   0      0   2   0      0  0.262057  0.323590  1.037564  0.106006   \n",
      "3      0   0      0   2   0      0  0.445841  0.220531  0.812120  0.000000   \n",
      "4      0   0      0   0   0      0 -0.971360 -1.241510  0.585408 -0.639572   \n",
      "...   ..  ..    ...  ..  ..    ...       ...       ...       ...       ...   \n",
      "3418   0   0      0   0   0      1 -0.917662 -1.298313 -1.879649 -0.565504   \n",
      "3419   0   0      0   2   0      0  0.479167  0.324590  0.349936  0.000000   \n",
      "3420   0   0      0   0   0      0 -0.515278 -0.679934 -0.416422 -1.104080   \n",
      "3421   0   0      0   0   0      0  0.285715  0.070415 -1.550311  0.000000   \n",
      "3422   0   0      0   0   0      0  0.102410 -0.399898 -0.285076  0.000000   \n",
      "\n",
      "         SM6_B            nX  \n",
      "0     0.139808 -1.602862e-01  \n",
      "1     1.289686  2.612766e-17  \n",
      "2     0.321156 -1.602862e-01  \n",
      "3    -0.044913 -1.602862e-01  \n",
      "4    -0.955872 -1.602862e-01  \n",
      "...        ...           ...  \n",
      "3418 -0.992050 -1.602862e-01  \n",
      "3419  0.171440 -1.602862e-01  \n",
      "3420 -1.153264 -1.602862e-01  \n",
      "3421 -0.130313 -1.602862e-01  \n",
      "3422 -0.482184 -1.602862e-01  \n",
      "\n",
      "[3423 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "X_train_rf = X_train_total[X_train_total.columns[(sel.get_support())]]\n",
    "X_test_rf = X_test_total[X_test_total.columns[(sel.get_support())]]\n",
    "\n",
    "print(X_train_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a Random Forest Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy is:  0.9658\n",
      "The Precision is:  0.9856\n",
      "The Recall is:  0.9745\n",
      "The F1 score is:  0.9800\n",
      "The Matthews correlation coefficient is:  0.8629\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  146   14\n",
      "1   25  956\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators = 100)\n",
    "rf_model.fit(X_train_rf, y_train_total)\n",
    "\n",
    "preds = rf_model.predict(X_test_rf)\n",
    "truths = y_test_total.to_numpy()\n",
    "\n",
    "printClassResults(preds,truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Random Forest Model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9460803502501417---(127, 'log_loss', None, 2, 1, 0, 'sqrt', None, 0, True, False, None, None, 0, True, None, 0, None)\n",
      "0.9460803502501417---(129, 'entropy', None, 2, 1, 0, 'sqrt', None, 0, True, False, None, None, 0, False, None, 0, None)\n",
      "Max Fitness = 0.8808272270638579 using the mean of ['F1', 'MatthewsCorrelation', 'Accuracy']\n",
      "Best Hyperparams = {'n_estimators': 129, 'criterion': 'log_loss', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_weight_fraction_leaf': 0, 'max_features': 'log2', 'max_leaf_nodes': None, 'min_impurity_decrease': 0, 'bootstrap': True, 'oob_score': True, 'n_jobs': None, 'random_state': None, 'verbose': 0, 'warm_start': False, 'class_weight': None, 'ccp_alpha': 0, 'max_samples': None}\n",
      "The Accuracy is:  0.9728\n",
      "The Precision is:  0.9897\n",
      "The Recall is:  0.9786\n",
      "The F1 score is:  0.9841\n",
      "The Matthews correlation coefficient is:  0.8911\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  150   10\n",
      "1   21  960\n"
     ]
    }
   ],
   "source": [
    "def rf_optimization(X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame, target_classes: list = [\"F1\"]) -> Tuple[RandomForestClassifier, np.array]:\n",
    "    \n",
    "    assert \"F1\" in target_classes or \"Accuracy\" in target_classes or \"Precision\" in target_classes or \"Recall\" in target_classes or \"MatthewsCorrelation\" in target_classes\n",
    "\n",
    "    #n_estimators = [100]\n",
    "    #n_estimators = [110,120,130,150,200]\n",
    "    #n_estimators = [110,115,120,125,130]\n",
    "    n_estimators = [125,126,127,128,129,130]\n",
    "    #criterion = [\"gini\"]\n",
    "    criterion = [\"gini\",\"entropy\",\"log_loss\"]\n",
    "    max_depth = [None]\n",
    "    min_samples_split = [2]\n",
    "    min_samples_leaf = [1]\n",
    "    min_weight_fraction_leaf = [0]\n",
    "    #max_features = [\"sqrt\"]\n",
    "    max_features = [\"sqrt\",\"log2\",None]\n",
    "    max_leaf_nodes = [None]\n",
    "    min_impurity_decrease = [0]\n",
    "    #bootstrap = [True]\n",
    "    bootstrap = [True,False]\n",
    "    #oob_score = [False]\n",
    "    oob_score = [False,True]\n",
    "    n_jobs = [None]\n",
    "    random_state = [None]\n",
    "    verbose = [0]\n",
    "    #warm_start = [False]\n",
    "    warm_start = [False,True]\n",
    "    class_weight = [None]\n",
    "    ccp_alpha = [0]\n",
    "    max_samples = [None]\n",
    "\n",
    "    rf_hyperparams = itertools.product(\n",
    "                                        n_estimators,\n",
    "                                        criterion,\n",
    "                                        max_depth,\n",
    "                                        min_samples_split,\n",
    "                                        min_samples_leaf,\n",
    "                                        min_weight_fraction_leaf,\n",
    "                                        max_features,\n",
    "                                        max_leaf_nodes,\n",
    "                                        min_impurity_decrease,\n",
    "                                        bootstrap,\n",
    "                                        oob_score,\n",
    "                                        n_jobs,\n",
    "                                        random_state,\n",
    "                                        verbose,\n",
    "                                        warm_start,\n",
    "                                        class_weight,\n",
    "                                        ccp_alpha,\n",
    "                                        max_samples\n",
    "                                    )\n",
    "\n",
    "    best_fitness = -99999\n",
    "    best_model = None\n",
    "    best_hyper_params = None\n",
    "\n",
    "    for hyper_param in rf_hyperparams:\n",
    "\n",
    "        n_estimators,\\\n",
    "            criterion,\\\n",
    "            max_depth,\\\n",
    "            min_samples_split,\\\n",
    "            min_samples_leaf,\\\n",
    "            min_weight_fraction_leaf,\\\n",
    "            max_features,\\\n",
    "            max_leaf_nodes,\\\n",
    "            min_impurity_decrease,\\\n",
    "            bootstrap,\\\n",
    "            oob_score,\\\n",
    "            n_jobs,\\\n",
    "            random_state,\\\n",
    "            verbose,\\\n",
    "            warm_start,\\\n",
    "            class_weight,\\\n",
    "            ccp_alpha,\\\n",
    "            max_samples\\\n",
    "                = hyper_param\n",
    "\n",
    "        if oob_score and not bootstrap:\n",
    "            continue\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "                        n_estimators=n_estimators,\n",
    "                        criterion=criterion,\n",
    "                        max_depth=max_depth,\n",
    "                        min_samples_split=min_samples_split,\n",
    "                        min_samples_leaf=min_samples_leaf,\n",
    "                        min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                        max_features=max_features,\n",
    "                        max_leaf_nodes=max_leaf_nodes,\n",
    "                        min_impurity_decrease=min_impurity_decrease,\n",
    "                        bootstrap=bootstrap,\n",
    "                        oob_score=oob_score,\n",
    "                        n_jobs=n_jobs,\n",
    "                        random_state=random_state,\n",
    "                        verbose=verbose,\n",
    "                        warm_start=warm_start,\n",
    "                        class_weight=class_weight,\n",
    "                        ccp_alpha=ccp_alpha,\n",
    "                        max_samples=max_samples\n",
    "                    )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        preds = model.predict(X_test)\n",
    "        truths = y_test.to_numpy()\n",
    "\n",
    "        fitness = 0\n",
    "\n",
    "        for target_class in target_classes:\n",
    "            if target_class == \"F1\":\n",
    "                fitness += f1_score(truths, preds, pos_label='RB')\n",
    "            elif target_class == \"Accuracy\":\n",
    "                fitness += accuracy_score(truths, preds)\n",
    "            elif target_class == \"Precision\":\n",
    "                fitness += precision_score(truths, preds, pos_label='RB')\n",
    "            elif target_class == \"Recall\":\n",
    "                fitness += recall_score(truths, preds, pos_label='RB')\n",
    "            elif target_class == \"MatthewsCorrelation\":\n",
    "                fitness += matthews_corrcoef(truths, preds)\n",
    "            else:\n",
    "                raise Exception(f\"Unkown Metric {target_class}\")\n",
    "        \n",
    "        fitness = fitness / len(target_classes)\n",
    "\n",
    "        if fitness > best_fitness:\n",
    "            best_model = model\n",
    "            best_fitness = fitness\n",
    "            best_hyper_params = hyper_param\n",
    "        elif fitness == best_fitness:\n",
    "            print(str(fitness) + \"---\" + str(hyper_param))\n",
    "\n",
    "    \n",
    "    print(f\"Max Fitness = {str(fitness)} using the mean of {str(target_classes)}\")\n",
    "\n",
    "    n_estimators,\\\n",
    "            criterion,\\\n",
    "            max_depth,\\\n",
    "            min_samples_split,\\\n",
    "            min_samples_leaf,\\\n",
    "            min_weight_fraction_leaf,\\\n",
    "            max_features,\\\n",
    "            max_leaf_nodes,\\\n",
    "            min_impurity_decrease,\\\n",
    "            bootstrap,\\\n",
    "            oob_score,\\\n",
    "            n_jobs,\\\n",
    "            random_state,\\\n",
    "            verbose,\\\n",
    "            warm_start,\\\n",
    "            class_weight,\\\n",
    "            ccp_alpha,\\\n",
    "            max_samples\\\n",
    "                = best_hyper_params\n",
    "\n",
    "    best_hyper_params_dict = {\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"criterion\": criterion,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"min_samples_split\": min_samples_split,\n",
    "        \"min_samples_leaf\": min_samples_leaf,\n",
    "        \"min_weight_fraction_leaf\": min_weight_fraction_leaf,\n",
    "        \"max_features\": max_features,\n",
    "        \"max_leaf_nodes\": max_leaf_nodes,\n",
    "        \"min_impurity_decrease\": min_impurity_decrease,\n",
    "        \"bootstrap\": bootstrap,\n",
    "        \"oob_score\": oob_score,\n",
    "        \"n_jobs\": n_jobs,\n",
    "        \"random_state\": random_state,\n",
    "        \"verbose\": verbose,\n",
    "        \"warm_start\": warm_start,\n",
    "        \"class_weight\": class_weight,\n",
    "        \"ccp_alpha\": ccp_alpha,\n",
    "        \"max_samples\": max_samples\n",
    "    }\n",
    "\n",
    "    print(f\"Best Hyperparams = {str(best_hyper_params_dict)}\")\n",
    "    return best_model, best_fitness, best_hyper_params_dict\n",
    "\n",
    "\n",
    "model, fitness, hyper_params_dict = rf_optimization(X_train_rf, y_train_total, X_test_rf, y_test_total, [\"F1\", \"MatthewsCorrelation\",\"Accuracy\"])\n",
    "\n",
    "preds = model.predict(X_test_rf)\n",
    "truths = y_test_total.to_numpy()\n",
    "\n",
    "printClassResults(preds,truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy is:  0.9544\n",
      "The Precision is:  0.9907\n",
      "The Recall is:  0.9572\n",
      "The F1 score is:  0.9737\n",
      "The Matthews correlation coefficient is:  0.8118\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  128    9\n",
      "1   43  961\n"
     ]
    }
   ],
   "source": [
    "svc_model = SVC()\n",
    "svc_model.fit(X_train_rf, y_train_total)\n",
    "\n",
    "preds = svc_model.predict(X_test_rf)\n",
    "truths = y_test_total.to_numpy()\n",
    "\n",
    "printClassResults(preds,truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Support Vector Machines for Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8812861026843137---(1, 'linear', 3, 'scale', 0, True, False, 0.001, 200, None, False, -1, 'ovo', False, None)\n",
      "0.8812861026843137---(1, 'linear', 3, 'scale', 0, True, True, 0.001, 200, None, False, -1, 'ovr', False, None)\n",
      "0.8812861026843137---(1, 'linear', 3, 'scale', 0, True, True, 0.001, 200, None, False, -1, 'ovo', False, None)\n",
      "0.8812861026843137---(1, 'linear', 3, 'scale', 0, False, False, 0.001, 200, None, False, -1, 'ovr', False, None)\n",
      "0.8812861026843137---(1, 'linear', 3, 'scale', 0, False, False, 0.001, 200, None, False, -1, 'ovo', False, None)\n",
      "0.8812861026843137---(1, 'linear', 3, 'scale', 0, False, True, 0.001, 200, None, False, -1, 'ovr', False, None)\n",
      "0.8812861026843137---(1, 'linear', 3, 'scale', 0, False, True, 0.001, 200, None, False, -1, 'ovo', False, None)\n",
      "0.913297452837417---(1, 'rbf', 3, 'scale', 0, True, False, 0.001, 200, None, False, -1, 'ovo', False, None)\n",
      "0.913297452837417---(1, 'rbf', 3, 'scale', 0, True, True, 0.001, 200, None, False, -1, 'ovr', False, None)\n",
      "0.913297452837417---(1, 'rbf', 3, 'scale', 0, True, True, 0.001, 200, None, False, -1, 'ovo', False, None)\n",
      "0.913297452837417---(1, 'rbf', 3, 'scale', 0, False, False, 0.001, 200, None, False, -1, 'ovr', False, None)\n",
      "0.913297452837417---(1, 'rbf', 3, 'scale', 0, False, False, 0.001, 200, None, False, -1, 'ovo', False, None)\n",
      "0.913297452837417---(1, 'rbf', 3, 'scale', 0, False, True, 0.001, 200, None, False, -1, 'ovr', False, None)\n",
      "0.913297452837417---(1, 'rbf', 3, 'scale', 0, False, True, 0.001, 200, None, False, -1, 'ovo', False, None)\n",
      "0.9253500968368934---(4, 'rbf', 3, 'scale', 0, True, False, 0.001, 200, None, False, -1, 'ovo', False, None)\n",
      "0.9253500968368934---(4, 'rbf', 3, 'scale', 0, True, True, 0.001, 200, None, False, -1, 'ovr', False, None)\n",
      "0.9253500968368934---(4, 'rbf', 3, 'scale', 0, True, True, 0.001, 200, None, False, -1, 'ovo', False, None)\n",
      "0.9253500968368934---(4, 'rbf', 3, 'scale', 0, False, False, 0.001, 200, None, False, -1, 'ovr', False, None)\n",
      "0.9253500968368934---(4, 'rbf', 3, 'scale', 0, False, False, 0.001, 200, None, False, -1, 'ovo', False, None)\n",
      "0.9253500968368934---(4, 'rbf', 3, 'scale', 0, False, True, 0.001, 200, None, False, -1, 'ovr', False, None)\n",
      "0.9253500968368934---(4, 'rbf', 3, 'scale', 0, False, True, 0.001, 200, None, False, -1, 'ovo', False, None)\n",
      "Max Fitness = 0.9220422822657329 using the mean of ['F1', 'MatthewsCorrelation', 'Accuracy']\n",
      "Best Hyperparams = {'C': 4, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale', 'coef0': 0, 'shrinking': True, 'probability': False, 'tol': False, 'cache_size': 200, 'class_weight': None, 'verbose': False, 'max_iter': -1, 'decision_function_shape': 'ovr', 'break_ties': False, 'random_state': None}\n",
      "The Accuracy is:  0.9606\n",
      "The Precision is:  0.9928\n",
      "The Recall is:  0.9620\n",
      "The F1 score is:  0.9772\n",
      "The Matthews correlation coefficient is:  0.8383\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  133    7\n",
      "1   38  963\n"
     ]
    }
   ],
   "source": [
    "def svm_optimization(X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame, target_classes: list = [\"F1\"]) -> Tuple[RandomForestClassifier, np.array]:\n",
    "    \n",
    "    assert \"F1\" in target_classes or \"Accuracy\" in target_classes or \"Precision\" in target_classes or \"Recall\" in target_classes or \"MatthewsCorrelation\" in target_classes\n",
    "\n",
    "    #C = [1]\n",
    "    C = [1,4,6,8,12]\n",
    "    #kernel = [\"rbf\"]\n",
    "    #kernel = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']\n",
    "    kernel = ['linear', 'poly', 'rbf']\n",
    "    degree = [3]\n",
    "    gamma = [\"scale\"]\n",
    "    coef0 = [0]\n",
    "    #shrinking = [True]\n",
    "    shrinking = [True,False]\n",
    "    #probability = [False]\n",
    "    probability = [False, True]\n",
    "    tol = [0.001]\n",
    "    cache_size = [200]\n",
    "    class_weight = [None]\n",
    "    verbose = [False]\n",
    "    max_iter = [-1]\n",
    "    #decision_function_shape = [\"ovr\"]\n",
    "    decision_function_shape = [\"ovr\", 'ovo']\n",
    "    break_ties = [False]\n",
    "    random_state = [None]\n",
    "\n",
    "    rf_hyperparams = itertools.product(\n",
    "                                        C,\\\n",
    "                                        kernel,\\\n",
    "                                        degree,\\\n",
    "                                        gamma,\\\n",
    "                                        coef0,\\\n",
    "                                        shrinking,\\\n",
    "                                        probability,\\\n",
    "                                        tol,\\\n",
    "                                        cache_size,\\\n",
    "                                        class_weight,\\\n",
    "                                        verbose,\\\n",
    "                                        max_iter,\\\n",
    "                                        decision_function_shape,\\\n",
    "                                        break_ties,\\\n",
    "                                        random_state\n",
    "                                    )\n",
    "\n",
    "    best_fitness = -99999\n",
    "    best_model = None\n",
    "    best_hyper_params = None\n",
    "\n",
    "    for hyper_param in rf_hyperparams:\n",
    "\n",
    "        C,\\\n",
    "            kernel,\\\n",
    "            degree,\\\n",
    "            gamma,\\\n",
    "            coef0,\\\n",
    "            shrinking,\\\n",
    "            probability,\\\n",
    "            tol,\\\n",
    "            cache_size,\\\n",
    "            class_weight,\\\n",
    "            verbose,\\\n",
    "            max_iter,\\\n",
    "            decision_function_shape,\\\n",
    "            break_ties,\\\n",
    "            random_state\\\n",
    "                = hyper_param\n",
    "\n",
    "        model = SVC(\n",
    "                        C=C,\\\n",
    "                        kernel=kernel,\\\n",
    "                        degree=degree,\\\n",
    "                        gamma=gamma,\\\n",
    "                        coef0=coef0,\\\n",
    "                        shrinking=shrinking,\\\n",
    "                        probability=probability,\\\n",
    "                        tol=tol,\\\n",
    "                        cache_size=cache_size,\\\n",
    "                        class_weight=class_weight,\\\n",
    "                        verbose=verbose,\\\n",
    "                        max_iter=max_iter,\\\n",
    "                        decision_function_shape=decision_function_shape,\\\n",
    "                        break_ties=break_ties,\\\n",
    "                        random_state=random_state\n",
    "                    )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        preds = model.predict(X_test)\n",
    "        truths = y_test.to_numpy()\n",
    "\n",
    "        fitness = 0\n",
    "\n",
    "        for target_class in target_classes:\n",
    "            if target_class == \"F1\":\n",
    "                fitness += f1_score(truths, preds, pos_label='RB')\n",
    "            elif target_class == \"Accuracy\":\n",
    "                fitness += accuracy_score(truths, preds)\n",
    "            elif target_class == \"Precision\":\n",
    "                fitness += precision_score(truths, preds, pos_label='RB')\n",
    "            elif target_class == \"Recall\":\n",
    "                fitness += recall_score(truths, preds, pos_label='RB')\n",
    "            elif target_class == \"MatthewsCorrelation\":\n",
    "                fitness += matthews_corrcoef(truths, preds)\n",
    "            else:\n",
    "                raise Exception(f\"Unkown Metric {target_class}\")\n",
    "        \n",
    "        fitness = fitness / len(target_classes)\n",
    "\n",
    "        if fitness > best_fitness:\n",
    "            best_model = model\n",
    "            best_fitness = fitness\n",
    "            best_hyper_params = hyper_param\n",
    "        elif fitness == best_fitness:\n",
    "            print(str(fitness) + \"---\" + str(hyper_param))\n",
    "\n",
    "    \n",
    "    print(f\"Max Fitness = {str(fitness)} using the mean of {str(target_classes)}\")\n",
    "\n",
    "    C,\\\n",
    "        kernel,\\\n",
    "        degree,\\\n",
    "        gamma,\\\n",
    "        coef0,\\\n",
    "        shrinking,\\\n",
    "        probability,\\\n",
    "        tol,\\\n",
    "        cache_size,\\\n",
    "        class_weight,\\\n",
    "        verbose,\\\n",
    "        max_iter,\\\n",
    "        decision_function_shape,\\\n",
    "        break_ties,\\\n",
    "        random_state\\\n",
    "                = best_hyper_params\n",
    "\n",
    "    best_hyper_params_dict = {\n",
    "        \"C\": C,\n",
    "        \"kernel\": kernel,\n",
    "        \"degree\": degree,\n",
    "        \"gamma\": gamma,\n",
    "        \"coef0\": coef0,\n",
    "        \"shrinking\": shrinking,\n",
    "        \"probability\": probability,\n",
    "        \"tol\": probability,\n",
    "        \"cache_size\": cache_size,\n",
    "        \"class_weight\": class_weight,\n",
    "        \"verbose\": verbose,\n",
    "        \"max_iter\": max_iter,\n",
    "        \"decision_function_shape\": decision_function_shape,\n",
    "        \"break_ties\": break_ties,\n",
    "        \"random_state\": random_state\n",
    "    }\n",
    "\n",
    "    print(f\"Best Hyperparams = {str(best_hyper_params_dict)}\")\n",
    "    return best_model, best_fitness, best_hyper_params_dict\n",
    "\n",
    "\n",
    "model, fitness, hyper_params_dict = svm_optimization(X_train_rf, y_train_total, X_test_rf, y_test_total, [\"F1\", \"MatthewsCorrelation\",\"Accuracy\"])\n",
    "\n",
    "preds = model.predict(X_test_rf)\n",
    "truths = y_test_total.to_numpy()\n",
    "\n",
    "printClassResults(preds,truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes\n",
      "The Accuracy is:  0.9273\n",
      "The Precision is:  0.9515\n",
      "The Recall is:  0.9625\n",
      "The F1 score is:  0.9570\n",
      "The Matthews correlation coefficient is:  0.7224\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  135   47\n",
      "1   36  923\n",
      "\n",
      "\n",
      "Bernoulli Naive Bayes\n",
      "The Accuracy is:  0.8861\n",
      "The Precision is:  0.9227\n",
      "The Recall is:  0.9421\n",
      "The F1 score is:  0.9323\n",
      "The Matthews correlation coefficient is:  0.5747\n",
      "\n",
      "This is the Confusion Matrix\n",
      "     0    1\n",
      "0  116   75\n",
      "1   55  895\n"
     ]
    }
   ],
   "source": [
    "scaled = True # To use the models that don't work with scaled (negative) values.\n",
    "\n",
    "nb_model = GaussianNB() # Likelihood of the features is assumed to be Gaussian\n",
    "nb_model.fit(X_train_rf, y_train_total)\n",
    "\n",
    "preds = nb_model.predict(X_test_rf)\n",
    "truths = y_test_total.to_numpy()\n",
    "\n",
    "print(\"Gaussian Naive Bayes\")\n",
    "printClassResults(preds,truths)\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "if not scaled:\n",
    "    nb_model = MultinomialNB() # For multinomially distributed data\n",
    "    nb_model.fit(X_train_rf, y_train_total)\n",
    "\n",
    "    preds = nb_model.predict(X_test_rf)\n",
    "    truths = y_test_total.to_numpy()\n",
    "\n",
    "    print(\"Multinomial Naive Bayes\")\n",
    "    printClassResults(preds,truths)\n",
    "\n",
    "\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    # For imbalanced datasets - ours seems to be really imbalenced towards the category \"RB\", but doesn't work with the data scaling. Tested it without scaling, didn't surpass the Gaussian nor the Bernoulli, so we didn't further optimize this\n",
    "    nb_model = ComplementNB()\n",
    "    nb_model.fit(X_train_rf, y_train_total)\n",
    "\n",
    "    preds = nb_model.predict(X_test_rf)\n",
    "    truths = y_test_total.to_numpy()\n",
    "\n",
    "    print(\"Complement Naive Bayes\")\n",
    "    printClassResults(preds,truths)\n",
    "\n",
    "\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "nb_model = BernoulliNB() # Distributed according to multivariate Bernoulli distributions\n",
    "nb_model.fit(X_train_rf, y_train_total)\n",
    "\n",
    "preds = nb_model.predict(X_test_rf)\n",
    "truths = y_test_total.to_numpy()\n",
    "\n",
    "print(\"Bernoulli Naive Bayes\")\n",
    "printClassResults(preds,truths)\n",
    "\n",
    "\n",
    "\n",
    "if not scaled:\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    nb_model = CategoricalNB() # For categorically distributed data\n",
    "    nb_model.fit(X_train_rf, y_train_total)\n",
    "    \n",
    "    preds = nb_model.predict(X_test_rf)\n",
    "    truths = y_test_total.to_numpy()\n",
    "    \n",
    "    print(\"Categorical Naive Bayes\")\n",
    "    printClassResults(preds,truths)\n",
    "\n",
    "\n",
    "\n",
    "# Out-of-core with the partial_fit function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X-Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada-Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "## Features\n",
    "### Using Random Forest Classifier for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['nHM', 'F04', 'NssssC', 'nCb', 'F03', 'F02_CN', 'SpMax_L', 'SM6_L', 'SpPosA_B', 'SpMax_B', 'SM6_B', 'nX']\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_rf.columns))\n",
    "print(X_train_rf.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "### Random Forest Classifier\n",
    "Best Hyperparams = {'n_estimators': 129, 'criterion': 'log_loss', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_weight_fraction_leaf': 0, 'max_features': 'log2', 'max_leaf_nodes': None, 'min_impurity_decrease': 0, 'bootstrap': True, 'oob_score': True, 'n_jobs': None, 'random_state': None, 'verbose': 0, 'warm_start': False, 'class_weight': None, 'ccp_alpha': 0, 'max_samples': None}\n",
    "<br>\n",
    "<br>\n",
    "The **Accuracy** is:  **0.9728**\n",
    "<br>\n",
    "The **Precision** is:  **0.9897**\n",
    "<br>\n",
    "The **Recall** is:  **0.9786**\n",
    "<br>\n",
    "The **F1 score** is:  **0.9841**\n",
    "<br>\n",
    "The **Matthews correlation coefficient** is:  **0.8911**\n",
    "<br>\n",
    "<br>\n",
    "This is the **Confusion Matrix**\n",
    "\n",
    "|   | 0   | 1   |\n",
    "|---|-----|-----|\n",
    "| 0 | 150 | 10  |\n",
    "| 1 | 21  | 960 |\n",
    "\n",
    "### SVM for Classification (SVC)\n",
    "Best Hyperparams = {'C': 4, 'kernel': 'rbf', 'degree': 3, 'gamma': 'scale', 'coef0': 0, 'shrinking': True, 'probability': False, 'tol': False, 'cache_size': 200, 'class_weight': None, 'verbose': False, 'max_iter': -1, 'decision_function_shape': 'ovr', 'break_ties': False, 'random_state': None}\n",
    "<br>\n",
    "<br>\n",
    "The **Accuracy** is:  **0.9606**\n",
    "<br>\n",
    "The **Precision** is:  **0.9928**\n",
    "<br>\n",
    "The **Recall** is:  **0.9620**\n",
    "<br>\n",
    "The **F1 score** is:  **0.9772**\n",
    "<br>\n",
    "The **Matthews correlation coefficient** is:  **0.8383**\n",
    "<br>\n",
    "<br>\n",
    "This is the **Confusion Matrix**\n",
    "\n",
    "|   | 0   | 1   |\n",
    "|---|-----|-----|\n",
    "| 0 | 133 | 7   |\n",
    "| 1 | 38  | 963 |\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "### X-Boost\n",
    "\n",
    "### Ada-Boost\n",
    "\n",
    "### K-Nearest Neighbours\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "### Multi-Layer Perceptron\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "055142e5cf66ed8ee87e97d9d29944a505c8696ace08c4c3475747b47f85af0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
